{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UiYHuHEyGm0u",
    "outputId": "c4c70d94-49db-4227-bc51-37a084d4c810"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jan 15 10:21:18 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA L4                      Off | 00000000:00:03.0 Off |                    0 |\n",
      "| N/A   46C    P8              17W /  72W |      4MiB / 23034MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sj-ANovHTVhD",
    "outputId": "9a328b70-839a-4f03-b3c0-328da93d7c66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/assets/llm-reasoners_main\n"
     ]
    }
   ],
   "source": [
    "%cd /workspace/assets/llm-reasoners_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gl0JAZvGTjB2",
    "outputId": "679ed41f-15db-4b80-ad97-8c0f77e7c7ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Obtaining file:///workspace/assets/llm-reasoners_main\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting llama1@ git+https://github.com/AegeanYan/llama@llama_v1 (from reasoners==0.0.0)\n",
      "  Cloning https://github.com/AegeanYan/llama (to revision llama_v1) to /tmp/pip-install-ccko4icm/llama1_3726c2fe78bc42a39bba46465cec46f2\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/AegeanYan/llama /tmp/pip-install-ccko4icm/llama1_3726c2fe78bc42a39bba46465cec46f2\n",
      "  Running command git checkout -b llama_v1 --track origin/llama_v1\n",
      "  Switched to a new branch 'llama_v1'\n",
      "  Branch 'llama_v1' set up to track remote branch 'llama_v1' from 'origin'.\n",
      "  Resolved https://github.com/AegeanYan/llama to commit bbb5c8e861e25e073bc4d3356152b1ac8422aa09\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting llama@ git+https://github.com/facebookresearch/llama@main (from reasoners==0.0.0)\n",
      "  Cloning https://github.com/facebookresearch/llama (to revision main) to /tmp/pip-install-ccko4icm/llama_741fb42eac2f4c79893987e7807e0012\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/llama /tmp/pip-install-ccko4icm/llama_741fb42eac2f4c79893987e7807e0012\n",
      "  Resolved https://github.com/facebookresearch/llama to commit ef351e9cd9496c579bf9f2bb036ef11bdc5ca3d2\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from reasoners==0.0.0) (4.65.0)\n",
      "Requirement already satisfied: fire in /usr/local/lib/python3.10/dist-packages (from reasoners==0.0.0) (0.5.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from reasoners==0.0.0) (1.26.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from reasoners==0.0.0) (1.11.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from reasoners==0.0.0) (2.1.0a0+b5021ba)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from reasoners==0.0.0) (2.14.7)\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from reasoners==0.0.0) (0.19.3)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from reasoners==0.0.0) (4.35.2)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from reasoners==0.0.0) (0.1.99)\n",
      "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (from reasoners==0.0.0) (1.3.0)\n",
      "Collecting tarski (from reasoners==0.0.0)\n",
      "  Downloading tarski-0.8.2-py3-none-any.whl (213 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.1/213.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (from reasoners==0.0.0) (0.6.2)\n",
      "Collecting optimum (from reasoners==0.0.0)\n",
      "  Downloading optimum-1.16.1-py3-none-any.whl (403 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m403.3/403.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from reasoners==0.0.0) (1.11.1)\n",
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (from reasoners==0.0.0) (0.41.2.post2)\n",
      "Collecting fairscale (from reasoners==0.0.0)\n",
      "  Downloading fairscale-0.4.13.tar.gz (266 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.3/266.3 kB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->reasoners==0.0.0) (11.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->reasoners==0.0.0) (0.5)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->reasoners==0.0.0) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->reasoners==0.0.0) (1.5.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets->reasoners==0.0.0) (2.31.0)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->reasoners==0.0.0) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->reasoners==0.0.0) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets->reasoners==0.0.0) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->reasoners==0.0.0) (3.8.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets->reasoners==0.0.0) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets->reasoners==0.0.0) (6.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->reasoners==0.0.0) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->reasoners==0.0.0) (4.7.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->reasoners==0.0.0) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->reasoners==0.0.0) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->reasoners==0.0.0) (3.1.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire->reasoners==0.0.0) (1.16.0)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->reasoners==0.0.0) (2.3.0)\n",
      "Requirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai->reasoners==0.0.0) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai->reasoners==0.0.0) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai->reasoners==0.0.0) (0.25.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai->reasoners==0.0.0) (1.10.11)\n",
      "Collecting coloredlogs (from optimum->reasoners==0.0.0)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m161.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft->reasoners==0.0.0) (5.9.4)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft->reasoners==0.0.0) (0.24.1)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft->reasoners==0.0.0) (0.4.0)\n",
      "Collecting multipledispatch (from tarski->reasoners==0.0.0)\n",
      "  Downloading multipledispatch-1.0.0-py3-none-any.whl (12 kB)\n",
      "Collecting antlr4-python3-runtime==4.7.2 (from tarski->reasoners==0.0.0)\n",
      "  Downloading antlr4-python3-runtime-4.7.2.tar.gz (112 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.3/112.3 kB\u001b[0m \u001b[31m289.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->reasoners==0.0.0) (2023.6.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers->reasoners==0.0.0) (0.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai->reasoners==0.0.0) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai->reasoners==0.0.0) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai->reasoners==0.0.0) (1.1.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->reasoners==0.0.0) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->reasoners==0.0.0) (3.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->reasoners==0.0.0) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->reasoners==0.0.0) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->reasoners==0.0.0) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->reasoners==0.0.0) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->reasoners==0.0.0) (1.3.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai->reasoners==0.0.0) (2023.5.7)\n",
      "Requirement already satisfied: httpcore in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai->reasoners==0.0.0) (1.0.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->reasoners==0.0.0) (1.26.16)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers->reasoners==0.0.0) (3.20.3)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->optimum->reasoners==0.0.0)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m269.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->reasoners==0.0.0) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->reasoners==0.0.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->reasoners==0.0.0) (2023.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->reasoners==0.0.0) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore->httpx<1,>=0.23.0->openai->reasoners==0.0.0) (0.14.0)\n",
      "Building wheels for collected packages: fairscale, llama, llama1, antlr4-python3-runtime\n",
      "  Building wheel for fairscale (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fairscale: filename=fairscale-0.4.13-py3-none-any.whl size=332126 sha256=e987cbafee805276c1ea4552ca1289ae715bddc6d37804aba95618a53edca788\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-okwu3q09/wheels/78/a4/c0/fb0a7ef03cff161611c3fa40c6cf898f76e58ec421b88e8cb3\n",
      "  Building wheel for llama (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llama: filename=llama-0.0.1-py3-none-any.whl size=14405 sha256=6088f9c67355cd5982c7d48dc7550868baeaa42872c462f289d32513277c7b24\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-okwu3q09/wheels/45/12/a6/4b0afb0097259ac4aa71be3eaf224bead3a32c3ac64fa61af7\n",
      "  Building wheel for llama1 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llama1: filename=llama1-0.0.0-py3-none-any.whl size=17918 sha256=423bf8fd8c210b4972997fd2143752881040e9e76d4e64d1140f9d9ec1c57320\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-okwu3q09/wheels/c8/44/d8/8e82fa63fbc38bbbbb9be3d6e2c514b67914e0ff635af71aee\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.7.2-py3-none-any.whl size=140949 sha256=2f230982dad0280f3dcf0465f8bf985f4eef21d98adc827e600b4650a9a3996d\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-okwu3q09/wheels/79/20/ec/30bf7dabc29319ccc0d0c96f910a640513a3c81faa960fed43\n",
      "Successfully built fairscale llama llama1 antlr4-python3-runtime\n",
      "Installing collected packages: multipledispatch, llama1, antlr4-python3-runtime, tarski, humanfriendly, coloredlogs, fairscale, llama, optimum, reasoners\n",
      "  Running setup.py develop for reasoners\n",
      "Successfully installed antlr4-python3-runtime-4.7.2 coloredlogs-15.0.1 fairscale-0.4.13 humanfriendly-10.0 llama-0.0.1 llama1-0.0.0 multipledispatch-1.0.0 optimum-1.16.1 reasoners-0.0.0 tarski-0.8.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "s1Qoqb9tCJro",
    "outputId": "f5f438c7-3ef2-4c71-dc09-d312d64666c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting git+https://github.com/facebookresearch/llama@llama_v1 (from -r requirements.txt (line 11))\n",
      "  Cloning https://github.com/facebookresearch/llama (to revision llama_v1) to /tmp/pip-req-build-_9ut33p5\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/llama /tmp/pip-req-build-_9ut33p5\n",
      "  Running command git checkout -b llama_v1 --track origin/llama_v1\n",
      "  Switched to a new branch 'llama_v1'\n",
      "  Branch 'llama_v1' set up to track remote branch 'llama_v1' from 'origin'.\n",
      "  Resolved https://github.com/facebookresearch/llama to commit 57b0eb62de0636e75af471e49e2f1862d908d9d8\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (4.65.0)\n",
      "Requirement already satisfied: fire in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (0.5.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (1.26.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (1.11.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (2.1.0a0+b5021ba)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (2.14.7)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (4.35.2)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (0.1.99)\n",
      "Requirement already satisfied: fairscale in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (0.4.13)\n",
      "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (1.3.0)\n",
      "Requirement already satisfied: requests~=2.31.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 16)) (2.31.0)\n",
      "Requirement already satisfied: PyYAML~=6.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 17)) (6.0)\n",
      "Requirement already satisfied: tarski~=0.8.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 18)) (0.8.2)\n",
      "Collecting setuptools~=67.8.0 (from -r requirements.txt (line 19))\n",
      "  Downloading setuptools-67.8.0-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting peft~=0.5.0 (from -r requirements.txt (line 23))\n",
      "  Downloading peft-0.5.0-py3-none-any.whl (85 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.6/85.6 kB\u001b[0m \u001b[31m259.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: optimum in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 24)) (1.16.1)\n",
      "Requirement already satisfied: ninja~=1.11.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 25)) (1.11.1)\n",
      "Requirement already satisfied: bitsandbytes~=0.41.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 26)) (0.41.2.post2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire->-r requirements.txt (line 2)) (1.16.0)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->-r requirements.txt (line 2)) (2.3.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 5)) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 5)) (4.7.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 5)) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 5)) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 5)) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 5)) (2023.6.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 6)) (11.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 6)) (0.5)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 6)) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 6)) (1.5.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 6)) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 6)) (0.70.15)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 6)) (3.8.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 6)) (0.19.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 6)) (23.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 8)) (2023.6.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 8)) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 8)) (0.4.0)\n",
      "Requirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai->-r requirements.txt (line 14)) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai->-r requirements.txt (line 14)) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai->-r requirements.txt (line 14)) (0.25.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai->-r requirements.txt (line 14)) (1.10.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests~=2.31.0->-r requirements.txt (line 16)) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests~=2.31.0->-r requirements.txt (line 16)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests~=2.31.0->-r requirements.txt (line 16)) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests~=2.31.0->-r requirements.txt (line 16)) (2023.5.7)\n",
      "Requirement already satisfied: multipledispatch in /usr/local/lib/python3.10/dist-packages (from tarski~=0.8.2->-r requirements.txt (line 18)) (1.0.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.7.2 in /usr/local/lib/python3.10/dist-packages (from tarski~=0.8.2->-r requirements.txt (line 18)) (4.7.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from tarski~=0.8.2->-r requirements.txt (line 18)) (5.9.4)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from peft~=0.5.0->-r requirements.txt (line 23)) (0.24.1)\n",
      "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from optimum->-r requirements.txt (line 24)) (15.0.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai->-r requirements.txt (line 14)) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai->-r requirements.txt (line 14)) (1.1.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 6)) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 6)) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 6)) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 6)) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 6)) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 6)) (1.3.1)\n",
      "Requirement already satisfied: httpcore in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 14)) (1.0.2)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 8)) (3.20.3)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->optimum->-r requirements.txt (line 24)) (10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r requirements.txt (line 5)) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r requirements.txt (line 6)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r requirements.txt (line 6)) (2023.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->-r requirements.txt (line 5)) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore->httpx<1,>=0.23.0->openai->-r requirements.txt (line 14)) (0.14.0)\n",
      "Building wheels for collected packages: llama\n",
      "  Building wheel for llama (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llama: filename=llama-0.0.0-py3-none-any.whl size=17896 sha256=59d01e2c89eb5c694a817264b1f6593395a9b464f60c363cd8f3ab7b45aad40d\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-nf6rhou_/wheels/2b/52/58/73f1ca5bfa254c328b923c8955c028e4403180909cb232e560\n",
      "Successfully built llama\n",
      "Installing collected packages: llama, setuptools, peft\n",
      "  Attempting uninstall: llama\n",
      "    Found existing installation: llama 0.0.1\n",
      "    Uninstalling llama-0.0.1:\n",
      "      Successfully uninstalled llama-0.0.1\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 68.0.0\n",
      "    Uninstalling setuptools-68.0.0:\n",
      "      Successfully uninstalled setuptools-68.0.0\n",
      "  Attempting uninstall: peft\n",
      "    Found existing installation: peft 0.6.2\n",
      "    Uninstalling peft-0.6.2:\n",
      "      Successfully uninstalled peft-0.6.2\n",
      "Successfully installed llama-0.0.0 peft-0.5.0 setuptools-67.8.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AaOu1Or7EZZf",
    "outputId": "b2bd5a4d-33c9-4980-e5eb-7cfad74cd55f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!pip install -q huggingface_hub\n",
    "import huggingface_hub\n",
    "huggingface_hub.login(token=\"token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rfF1fWLz2H9c",
    "outputId": "f96a963c-7d0d-4163-9da3-91bef57b4547"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace\n"
     ]
    }
   ],
   "source": [
    "%cd /workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3b08xXye3FFm",
    "outputId": "a099e220-c11e-4847-a008-67941d6defef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/assets/llm-reasoners_main\n"
     ]
    }
   ],
   "source": [
    "%cd /workspace/assets/llm-reasoners_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5OFzG6GmefJ4",
    "outputId": "2fa63496-dc3c-44af-fbf7-5a14d892117b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q pddl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "JuxmoAf9qS0i"
   },
   "outputs": [],
   "source": [
    "# prompt: 環境変数VALを設定する\n",
    "import os\n",
    "os.environ['VAL'] = '/workspace/assets/llm-reasoners_main/LLMs-Planning/planner_tools/VAL'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aW_7c6aDrpPp",
    "outputId": "676329b8-9a7a-430d-b937-f53d8f3b2dc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/assets/llm-reasoners_main/LLMs-Planning/planner_tools/VAL\n"
     ]
    }
   ],
   "source": [
    "# prompt: 環境変数VALの値を見る\n",
    "\n",
    "print(os.environ.get('VAL'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/assets/llm-reasoners_main\n"
     ]
    }
   ],
   "source": [
    "%cd /workspace/assets/llm-reasoners_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "FarIKkN6U6ka",
    "outputId": "b41394d6-c091-4692-e238-27b1bc6043f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantizing.............................\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:04<00:00,  2.14s/it]\n",
      "blocksworld:   0%|                                       | 0/47 [00:00<?, ?it/s]\n",
      "MCTS iteration:   0%|                                    | 0/10 [00:00<?, ?it/s]\u001b[A/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "\n",
      "MCTS iteration:  10%|██▊                         | 1/10 [01:32<13:53, 92.60s/it]\u001b[A\n",
      "                                                                                \u001b[A[+]: Saving plan in tmp_plan.txt\n",
      "RESPONSE::: Checking plan: tmp_plan.txt\n",
      "Plan executed successfully - checking goal\n",
      "Plan valid\n",
      "Final value: 2 \n",
      "\n",
      "Successful plans:\n",
      "Value: 2\n",
      " tmp_plan.txt 2 \n",
      "\n",
      "\n",
      "Case #1: correct=True, output='unstack the yellow block from on top of the orange block\\nstack the yellow block on top of the red block', answer={'init': 'the red block is clear, the yellow block is clear, the hand is empty, the orange block is on top of the blue block, the yellow block is on top of the orange block, the red block is on the table and the blue block is on the table', 'goal': 'the orange block is on top of the blue block and the yellow block is on top of the red block', 'plan': '\\nunstack the yellow block from on top of the orange block\\nstack the yellow block on top of the red block\\n[PLAN END]\\n', 'question': '\\n[STATEMENT]\\nAs initial conditions I have that, the red block is clear, the yellow block is clear, the hand is empty, the orange block is on top of the blue block, the yellow block is on top of the orange block, the red block is on the table and the blue block is on the table.\\nMy goal is to have that the orange block is on top of the blue block and the yellow block is on top of the red block.\\n\\nMy plan is as follows:\\n\\n[PLAN]\\n', 'instance_file': 'LLMs-Planning/llm_planning_analysis/instances/blocksworld/generated_basic/instance-192.pddl'};accuracy=1.000 (1/1)\n",
      "blocksworld:   2%|▌                            | 1/47 [01:33<1:12:02, 93.97s/it]\n",
      "MCTS iteration:   0%|                                    | 0/10 [00:00<?, ?it/s]\u001b[A/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Error: no successful change\n",
      "the blue block is in the hand\n",
      "['the blue block is clear', 'the orange block is clear', 'the yellow block is clear', 'the hand is empty', 'the yellow block is on top of the red block', 'the red block is on the table', 'the blue block is on the table', 'the orange block is on the table']\n",
      "/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "\n",
      "MCTS iteration:  10%|██▊                         | 1/10 [01:38<14:44, 98.27s/it]\u001b[A\n",
      "                                                                                \u001b[A[+]: Saving plan in tmp_plan.txt\n",
      "RESPONSE::: Checking plan: tmp_plan.txt\n",
      "Plan executed successfully - checking goal\n",
      "Plan valid\n",
      "Final value: 2 \n",
      "\n",
      "Successful plans:\n",
      "Value: 2\n",
      " tmp_plan.txt 2 \n",
      "\n",
      "\n",
      "Case #2: correct=True, output='pick up the blue block\\nstack the blue block on top of the orange block', answer={'init': 'the blue block is clear, the orange block is clear, the yellow block is clear, the hand is empty, the yellow block is on top of the red block, the red block is on the table, the blue block is on the table and the orange block is on the table', 'goal': 'the blue block is on top of the orange block and the yellow block is on top of the red block', 'plan': '\\npick up the blue block\\nstack the blue block on top of the orange block\\n[PLAN END]\\n', 'question': '\\n[STATEMENT]\\nAs initial conditions I have that, the blue block is clear, the orange block is clear, the yellow block is clear, the hand is empty, the yellow block is on top of the red block, the red block is on the table, the blue block is on the table and the orange block is on the table.\\nMy goal is to have that the blue block is on top of the orange block and the yellow block is on top of the red block.\\n\\nMy plan is as follows:\\n\\n[PLAN]\\n', 'instance_file': 'LLMs-Planning/llm_planning_analysis/instances/blocksworld/generated_basic/instance-149.pddl'};accuracy=1.000 (2/2)\n",
      "blocksworld:   4%|█▏                           | 2/47 [03:13<1:13:01, 97.38s/it]\n",
      "MCTS iteration:   0%|                                    | 0/10 [00:00<?, ?it/s]\u001b[A/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "\n",
      "MCTS iteration:  10%|██▊                         | 1/10 [01:32<13:49, 92.21s/it]\u001b[A/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "\n",
      "MCTS iteration:  30%|████████▍                   | 3/10 [03:00<06:37, 56.77s/it]\u001b[A/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "\n",
      "MCTS iteration:  90%|█████████████████████████▏  | 9/10 [04:31<00:25, 25.07s/it]\u001b[A\n",
      "                                                                                \u001b[A[+]: Saving plan in tmp_plan.txt\n",
      "RESPONSE::: Checking plan: tmp_plan.txt\n",
      "Plan executed successfully - checking goal\n",
      "Plan valid\n",
      "Final value: 2 \n",
      "\n",
      "Successful plans:\n",
      "Value: 2\n",
      " tmp_plan.txt 2 \n",
      "\n",
      "\n",
      "Case #3: correct=True, output='unstack the orange block from on top of the yellow block\\nstack the orange block on top of the blue block', answer={'init': 'the red block is clear, the blue block is clear, the orange block is clear, the hand is empty, the orange block is on top of the yellow block, the red block is on the table, the blue block is on the table and the yellow block is on the table', 'goal': 'the orange block is on top of the blue block', 'plan': '\\nunstack the orange block from on top of the yellow block\\nstack the orange block on top of the blue block\\n[PLAN END]\\n', 'question': '\\n[STATEMENT]\\nAs initial conditions I have that, the red block is clear, the blue block is clear, the orange block is clear, the hand is empty, the orange block is on top of the yellow block, the red block is on the table, the blue block is on the table and the yellow block is on the table.\\nMy goal is to have that the orange block is on top of the blue block.\\n\\nMy plan is as follows:\\n\\n[PLAN]\\n', 'instance_file': 'LLMs-Planning/llm_planning_analysis/instances/blocksworld/generated_basic/instance-396.pddl'};accuracy=1.000 (3/3)\n",
      "blocksworld:   6%|█▊                          | 3/47 [07:46<2:10:14, 177.61s/it]\n",
      "MCTS iteration:   0%|                                    | 0/10 [00:00<?, ?it/s]\u001b[A/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "\n",
      "MCTS iteration:  10%|██▊                         | 1/10 [01:29<13:28, 89.82s/it]\u001b[A\n",
      "                                                                                \u001b[A[+]: Saving plan in tmp_plan.txt\n",
      "RESPONSE::: Checking plan: tmp_plan.txt\n",
      "Plan executed successfully - checking goal\n",
      "Plan valid\n",
      "Final value: 2 \n",
      "\n",
      "Successful plans:\n",
      "Value: 2\n",
      " tmp_plan.txt 2 \n",
      "\n",
      "\n",
      "Case #4: correct=True, output='pick up the yellow block\\nstack the yellow block on top of the blue block', answer={'init': 'the blue block is clear, the yellow block is clear, the hand is empty, the blue block is on top of the orange block, the orange block is on top of the red block, the red block is on the table and the yellow block is on the table', 'goal': 'the blue block is on top of the orange block and the yellow block is on top of the blue block', 'plan': '\\npick up the yellow block\\nstack the yellow block on top of the blue block\\n[PLAN END]\\n', 'question': '\\n[STATEMENT]\\nAs initial conditions I have that, the blue block is clear, the yellow block is clear, the hand is empty, the blue block is on top of the orange block, the orange block is on top of the red block, the red block is on the table and the yellow block is on the table.\\nMy goal is to have that the blue block is on top of the orange block and the yellow block is on top of the blue block.\\n\\nMy plan is as follows:\\n\\n[PLAN]\\n', 'instance_file': 'LLMs-Planning/llm_planning_analysis/instances/blocksworld/generated_basic/instance-161.pddl'};accuracy=1.000 (4/4)\n",
      "blocksworld:   9%|██▍                         | 4/47 [09:18<1:42:52, 143.55s/it]\n",
      "MCTS iteration:   0%|                                    | 0/10 [00:00<?, ?it/s]\u001b[A/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "\n",
      "MCTS iteration:  10%|██▊                         | 1/10 [01:32<13:49, 92.17s/it]\u001b[A\n",
      "                                                                                \u001b[A[+]: Saving plan in tmp_plan.txt\n",
      "RESPONSE::: Checking plan: tmp_plan.txt\n",
      "Plan executed successfully - checking goal\n",
      "Plan valid\n",
      "Final value: 2 \n",
      "\n",
      "Successful plans:\n",
      "Value: 2\n",
      " tmp_plan.txt 2 \n",
      "\n",
      "\n",
      "Case #5: correct=True, output='unstack the red block from on top of the orange block\\nstack the red block on top of the yellow block', answer={'init': 'the red block is clear, the yellow block is clear, the hand is empty, the red block is on top of the orange block, the yellow block is on top of the blue block, the blue block is on the table and the orange block is on the table', 'goal': 'the red block is on top of the yellow block and the yellow block is on top of the blue block', 'plan': '\\nunstack the red block from on top of the orange block\\nstack the red block on top of the yellow block\\n[PLAN END]\\n', 'question': '\\n[STATEMENT]\\nAs initial conditions I have that, the red block is clear, the yellow block is clear, the hand is empty, the red block is on top of the orange block, the yellow block is on top of the blue block, the blue block is on the table and the orange block is on the table.\\nMy goal is to have that the red block is on top of the yellow block and the yellow block is on top of the blue block.\\n\\nMy plan is as follows:\\n\\n[PLAN]\\n', 'instance_file': 'LLMs-Planning/llm_planning_analysis/instances/blocksworld/generated_basic/instance-378.pddl'};accuracy=1.000 (5/5)\n",
      "blocksworld:  11%|██▉                         | 5/47 [10:52<1:27:57, 125.65s/it]\n",
      "MCTS iteration:   0%|                                    | 0/10 [00:00<?, ?it/s]\u001b[A/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Error: zero-colors\n",
      "i have that\n",
      "\n",
      "MCTS iteration:  10%|██▊                         | 1/10 [01:30<13:31, 90.12s/it]\u001b[A\n",
      "                                                                                \u001b[A[+]: Saving plan in tmp_plan.txt\n",
      "RESPONSE::: Checking plan: tmp_plan.txt\n",
      "Plan executed successfully - checking goal\n",
      "Goal not satisfied\n",
      "Plan invalid\n",
      "\n",
      "\n",
      "Failed plans:\n",
      " tmp_plan.txt \n",
      "\n",
      "Case #6: correct=False, output='pick up the blue block\\nstack the blue block on top of the red block', answer={'init': 'the red block is clear, the blue block is clear, the hand is empty, the red block is on top of the orange block, the orange block is on top of the yellow block, the blue block is on the table and the yellow block is on the table', 'goal': 'the red block is on top of the blue block', 'plan': '\\nunstack the red block from on top of the orange block\\nstack the red block on top of the blue block\\n[PLAN END]\\n', 'question': '\\n[STATEMENT]\\nAs initial conditions I have that, the red block is clear, the blue block is clear, the hand is empty, the red block is on top of the orange block, the orange block is on top of the yellow block, the blue block is on the table and the yellow block is on the table.\\nMy goal is to have that the red block is on top of the blue block.\\n\\nMy plan is as follows:\\n\\n[PLAN]\\n', 'instance_file': 'LLMs-Planning/llm_planning_analysis/instances/blocksworld/generated_basic/instance-273.pddl'};accuracy=0.833 (5/6)\n",
      "blocksworld:  13%|███▌                        | 6/47 [12:23<1:17:57, 114.07s/it]\n",
      "MCTS iteration:   0%|                                    | 0/10 [00:00<?, ?it/s]\u001b[A/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "\n",
      "MCTS iteration:  10%|██▊                         | 1/10 [01:33<14:00, 93.42s/it]\u001b[A\n",
      "                                                                                \u001b[A[+]: Saving plan in tmp_plan.txt\n",
      "RESPONSE::: Checking plan: tmp_plan.txt\n",
      "Plan executed successfully - checking goal\n",
      "Plan valid\n",
      "Final value: 2 \n",
      "\n",
      "Successful plans:\n",
      "Value: 2\n",
      " tmp_plan.txt 2 \n",
      "\n",
      "\n",
      "Case #7: correct=True, output='pick up the blue block\\nstack the blue block on top of the yellow block', answer={'init': 'the blue block is clear, the orange block is clear, the yellow block is clear, the hand is empty, the yellow block is on top of the red block, the red block is on the table, the blue block is on the table and the orange block is on the table', 'goal': 'the blue block is on top of the yellow block and the yellow block is on top of the red block', 'plan': '\\npick up the blue block\\nstack the blue block on top of the yellow block\\n[PLAN END]\\n', 'question': '\\n[STATEMENT]\\nAs initial conditions I have that, the blue block is clear, the orange block is clear, the yellow block is clear, the hand is empty, the yellow block is on top of the red block, the red block is on the table, the blue block is on the table and the orange block is on the table.\\nMy goal is to have that the blue block is on top of the yellow block and the yellow block is on top of the red block.\\n\\nMy plan is as follows:\\n\\n[PLAN]\\n', 'instance_file': 'LLMs-Planning/llm_planning_analysis/instances/blocksworld/generated_basic/instance-79.pddl'};accuracy=0.857 (6/7)\n",
      "blocksworld:  15%|████▏                       | 7/47 [13:58<1:11:53, 107.85s/it]\n",
      "MCTS iteration:   0%|                                    | 0/10 [00:00<?, ?it/s]\u001b[A/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "\n",
      "MCTS iteration:  10%|██▊                         | 1/10 [01:32<13:49, 92.21s/it]\u001b[A\n",
      "                                                                                \u001b[A[+]: Saving plan in tmp_plan.txt\n",
      "RESPONSE::: Checking plan: tmp_plan.txt\n",
      "Plan executed successfully - checking goal\n",
      "Plan valid\n",
      "Final value: 2 \n",
      "\n",
      "Successful plans:\n",
      "Value: 2\n",
      " tmp_plan.txt 2 \n",
      "\n",
      "\n",
      "Case #8: correct=True, output='pick up the orange block\\nstack the orange block on top of the red block', answer={'init': 'the red block is clear, the orange block is clear, the yellow block is clear, the hand is empty, the red block is on top of the blue block, the blue block is on the table, the orange block is on the table and the yellow block is on the table', 'goal': 'the orange block is on top of the red block', 'plan': '\\npick up the orange block\\nstack the orange block on top of the red block\\n[PLAN END]\\n', 'question': '\\n[STATEMENT]\\nAs initial conditions I have that, the red block is clear, the orange block is clear, the yellow block is clear, the hand is empty, the red block is on top of the blue block, the blue block is on the table, the orange block is on the table and the yellow block is on the table.\\nMy goal is to have that the orange block is on top of the red block.\\n\\nMy plan is as follows:\\n\\n[PLAN]\\n', 'instance_file': 'LLMs-Planning/llm_planning_analysis/instances/blocksworld/generated_basic/instance-124.pddl'};accuracy=0.875 (7/8)\n",
      "blocksworld:  17%|████▊                       | 8/47 [15:32<1:07:10, 103.34s/it]\n",
      "MCTS iteration:   0%|                                    | 0/10 [00:00<?, ?it/s]\u001b[A/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "\n",
      "MCTS iteration:  10%|██▊                         | 1/10 [01:32<13:50, 92.30s/it]\u001b[A\n",
      "                                                                                \u001b[A[+]: Saving plan in tmp_plan.txt\n",
      "RESPONSE::: Checking plan: tmp_plan.txt\n",
      "Plan executed successfully - checking goal\n",
      "Plan valid\n",
      "Final value: 2 \n",
      "\n",
      "Successful plans:\n",
      "Value: 2\n",
      " tmp_plan.txt 2 \n",
      "\n",
      "\n",
      "Case #9: correct=True, output='pick up the red block\\nstack the red block on top of the yellow block', answer={'init': 'the red block is clear, the blue block is clear, the yellow block is clear, the hand is empty, the blue block is on top of the orange block, the red block is on the table, the orange block is on the table and the yellow block is on the table', 'goal': 'the red block is on top of the yellow block and the blue block is on top of the orange block', 'plan': '\\npick up the red block\\nstack the red block on top of the yellow block\\n[PLAN END]\\n', 'question': '\\n[STATEMENT]\\nAs initial conditions I have that, the red block is clear, the blue block is clear, the yellow block is clear, the hand is empty, the blue block is on top of the orange block, the red block is on the table, the orange block is on the table and the yellow block is on the table.\\nMy goal is to have that the red block is on top of the yellow block and the blue block is on top of the orange block.\\n\\nMy plan is as follows:\\n\\n[PLAN]\\n', 'instance_file': 'LLMs-Planning/llm_planning_analysis/instances/blocksworld/generated_basic/instance-223.pddl'};accuracy=0.889 (8/9)\n",
      "blocksworld:  19%|█████▎                      | 9/47 [17:06<1:03:33, 100.37s/it]\n",
      "MCTS iteration:   0%|                                    | 0/10 [00:00<?, ?it/s]\u001b[A/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "\n",
      "MCTS iteration:  10%|██▊                         | 1/10 [01:30<13:30, 90.03s/it]\u001b[A\n",
      "                                                                                \u001b[A[+]: Saving plan in tmp_plan.txt\n",
      "RESPONSE::: Checking plan: tmp_plan.txt\n",
      "Plan executed successfully - checking goal\n",
      "Plan valid\n",
      "Final value: 2 \n",
      "\n",
      "Successful plans:\n",
      "Value: 2\n",
      " tmp_plan.txt 2 \n",
      "\n",
      "\n",
      "Case #10: correct=True, output='pick up the yellow block\\nstack the yellow block on top of the blue block', answer={'init': 'the blue block is clear, the yellow block is clear, the hand is empty, the red block is on top of the orange block, the blue block is on top of the red block, the orange block is on the table and the yellow block is on the table', 'goal': 'the red block is on top of the orange block and the yellow block is on top of the blue block', 'plan': '\\npick up the yellow block\\nstack the yellow block on top of the blue block\\n[PLAN END]\\n', 'question': '\\n[STATEMENT]\\nAs initial conditions I have that, the blue block is clear, the yellow block is clear, the hand is empty, the red block is on top of the orange block, the blue block is on top of the red block, the orange block is on the table and the yellow block is on the table.\\nMy goal is to have that the red block is on top of the orange block and the yellow block is on top of the blue block.\\n\\nMy plan is as follows:\\n\\n[PLAN]\\n', 'instance_file': 'LLMs-Planning/llm_planning_analysis/instances/blocksworld/generated_basic/instance-34.pddl'};accuracy=0.900 (9/10)\n",
      "blocksworld:  21%|█████▉                      | 10/47 [18:37<1:00:12, 97.63s/it]\n",
      "MCTS iteration:   0%|                                    | 0/10 [00:00<?, ?it/s]\u001b[A/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "\n",
      "MCTS iteration:  10%|██▊                         | 1/10 [01:32<13:55, 92.84s/it]\u001b[A\n",
      "                                                                                \u001b[A[+]: Saving plan in tmp_plan.txt\n",
      "RESPONSE::: Checking plan: tmp_plan.txt\n",
      "Plan executed successfully - checking goal\n",
      "Plan valid\n",
      "Final value: 2 \n",
      "\n",
      "Successful plans:\n",
      "Value: 2\n",
      " tmp_plan.txt 2 \n",
      "\n",
      "\n",
      "Case #11: correct=True, output='pick up the yellow block\\nstack the yellow block on top of the red block', answer={'init': 'the red block is clear, the orange block is clear, the yellow block is clear, the hand is empty, the orange block is on top of the blue block, the red block is on the table, the blue block is on the table and the yellow block is on the table', 'goal': 'the orange block is on top of the blue block and the yellow block is on top of the red block', 'plan': '\\npick up the yellow block\\nstack the yellow block on top of the red block\\n[PLAN END]\\n', 'question': '\\n[STATEMENT]\\nAs initial conditions I have that, the red block is clear, the orange block is clear, the yellow block is clear, the hand is empty, the orange block is on top of the blue block, the red block is on the table, the blue block is on the table and the yellow block is on the table.\\nMy goal is to have that the orange block is on top of the blue block and the yellow block is on top of the red block.\\n\\nMy plan is as follows:\\n\\n[PLAN]\\n', 'instance_file': 'LLMs-Planning/llm_planning_analysis/instances/blocksworld/generated_basic/instance-71.pddl'};accuracy=0.909 (10/11)\n",
      "blocksworld:  23%|███████                       | 11/47 [20:12<57:58, 96.63s/it]\n",
      "MCTS iteration:   0%|                                    | 0/10 [00:00<?, ?it/s]\u001b[A/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "\n",
      "MCTS iteration:  10%|██▊                         | 1/10 [01:29<13:29, 89.90s/it]\u001b[A\n",
      "                                                                                \u001b[A[+]: Saving plan in tmp_plan.txt\n",
      "RESPONSE::: Checking plan: tmp_plan.txt\n",
      "Plan executed successfully - checking goal\n",
      "Plan valid\n",
      "Final value: 2 \n",
      "\n",
      "Successful plans:\n",
      "Value: 2\n",
      " tmp_plan.txt 2 \n",
      "\n",
      "\n",
      "Case #12: correct=True, output='pick up the yellow block\\nstack the yellow block on top of the red block', answer={'init': 'the red block is clear, the yellow block is clear, the hand is empty, the red block is on top of the blue block, the blue block is on top of the orange block, the orange block is on the table and the yellow block is on the table', 'goal': 'the blue block is on top of the orange block and the yellow block is on top of the red block', 'plan': '\\npick up the yellow block\\nstack the yellow block on top of the red block\\n[PLAN END]\\n', 'question': '\\n[STATEMENT]\\nAs initial conditions I have that, the red block is clear, the yellow block is clear, the hand is empty, the red block is on top of the blue block, the blue block is on top of the orange block, the orange block is on the table and the yellow block is on the table.\\nMy goal is to have that the blue block is on top of the orange block and the yellow block is on top of the red block.\\n\\nMy plan is as follows:\\n\\n[PLAN]\\n', 'instance_file': 'LLMs-Planning/llm_planning_analysis/instances/blocksworld/generated_basic/instance-224.pddl'};accuracy=0.917 (11/12)\n",
      "blocksworld:  26%|███████▋                      | 12/47 [21:43<55:27, 95.06s/it]\n",
      "MCTS iteration:   0%|                                    | 0/10 [00:00<?, ?it/s]\u001b[A/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "\n",
      "MCTS iteration:  10%|██▊                         | 1/10 [01:29<13:26, 89.66s/it]\u001b[A\n",
      "                                                                                \u001b[A[+]: Saving plan in tmp_plan.txt\n",
      "RESPONSE::: Checking plan: tmp_plan.txt\n",
      "Plan executed successfully - checking goal\n",
      "Plan valid\n",
      "Final value: 2 \n",
      "\n",
      "Successful plans:\n",
      "Value: 2\n",
      " tmp_plan.txt 2 \n",
      "\n",
      "\n",
      "Case #13: correct=True, output='pick up the yellow block\\nstack the yellow block on top of the blue block', answer={'init': 'the blue block is clear, the yellow block is clear, the hand is empty, the red block is on top of the orange block, the blue block is on top of the red block, the orange block is on the table and the yellow block is on the table', 'goal': 'the blue block is on top of the red block and the yellow block is on top of the blue block', 'plan': '\\npick up the yellow block\\nstack the yellow block on top of the blue block\\n[PLAN END]\\n', 'question': '\\n[STATEMENT]\\nAs initial conditions I have that, the blue block is clear, the yellow block is clear, the hand is empty, the red block is on top of the orange block, the blue block is on top of the red block, the orange block is on the table and the yellow block is on the table.\\nMy goal is to have that the blue block is on top of the red block and the yellow block is on top of the blue block.\\n\\nMy plan is as follows:\\n\\n[PLAN]\\n', 'instance_file': 'LLMs-Planning/llm_planning_analysis/instances/blocksworld/generated_basic/instance-428.pddl'};accuracy=0.923 (12/13)\n",
      "blocksworld:  28%|████████▎                     | 13/47 [23:14<53:12, 93.90s/it]\n",
      "MCTS iteration:   0%|                                    | 0/10 [00:00<?, ?it/s]\u001b[A/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "\n",
      "MCTS iteration:  10%|██▊                         | 1/10 [01:32<13:49, 92.11s/it]\u001b[A\n",
      "                                                                                \u001b[A[+]: Saving plan in tmp_plan.txt\n",
      "RESPONSE::: Checking plan: tmp_plan.txt\n",
      "Plan executed successfully - checking goal\n",
      "Plan valid\n",
      "Final value: 2 \n",
      "\n",
      "Successful plans:\n",
      "Value: 2\n",
      " tmp_plan.txt 2 \n",
      "\n",
      "\n",
      "Case #14: correct=True, output='unstack the orange block from on top of the blue block\\nstack the orange block on top of the red block', answer={'init': 'the red block is clear, the orange block is clear, the hand is empty, the red block is on top of the yellow block, the orange block is on top of the blue block, the blue block is on the table and the yellow block is on the table', 'goal': 'the red block is on top of the yellow block and the orange block is on top of the red block', 'plan': '\\nunstack the orange block from on top of the blue block\\nstack the orange block on top of the red block\\n[PLAN END]\\n', 'question': '\\n[STATEMENT]\\nAs initial conditions I have that, the red block is clear, the orange block is clear, the hand is empty, the red block is on top of the yellow block, the orange block is on top of the blue block, the blue block is on the table and the yellow block is on the table.\\nMy goal is to have that the red block is on top of the yellow block and the orange block is on top of the red block.\\n\\nMy plan is as follows:\\n\\n[PLAN]\\n', 'instance_file': 'LLMs-Planning/llm_planning_analysis/instances/blocksworld/generated_basic/instance-426.pddl'};accuracy=0.929 (13/14)\n",
      "blocksworld:  30%|████████▉                     | 14/47 [24:48<51:36, 93.82s/it]\n",
      "MCTS iteration:   0%|                                    | 0/10 [00:00<?, ?it/s]\u001b[A/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Error: zero-colors\n",
      "i am not sure what the change is\n",
      "\n",
      "MCTS iteration:  10%|██▊                         | 1/10 [01:29<13:29, 89.99s/it]\u001b[A\n",
      "                                                                                \u001b[A[+]: Saving plan in tmp_plan.txt\n",
      "RESPONSE::: Checking plan: tmp_plan.txt\n",
      "Plan executed successfully - checking goal\n",
      "Plan valid\n",
      "Final value: 2 \n",
      "\n",
      "Successful plans:\n",
      "Value: 2\n",
      " tmp_plan.txt 2 \n",
      "\n",
      "\n",
      "Case #15: correct=True, output='pick up the red block\\nstack the red block on top of the blue block', answer={'init': 'the red block is clear, the blue block is clear, the hand is empty, the blue block is on top of the yellow block, the yellow block is on top of the white block, the white block is on top of the orange block, the red block is on the table and the orange block is on the table', 'goal': 'the red block is on top of the blue block and the blue block is on top of the yellow block', 'plan': '\\npick up the red block\\nstack the red block on top of the blue block\\n[PLAN END]\\n', 'question': '\\n[STATEMENT]\\nAs initial conditions I have that, the red block is clear, the blue block is clear, the hand is empty, the blue block is on top of the yellow block, the yellow block is on top of the white block, the white block is on top of the orange block, the red block is on the table and the orange block is on the table.\\nMy goal is to have that the red block is on top of the blue block and the blue block is on top of the yellow block.\\n\\nMy plan is as follows:\\n\\n[PLAN]\\n', 'instance_file': 'LLMs-Planning/llm_planning_analysis/instances/blocksworld/generated_basic/instance-495.pddl'};accuracy=0.933 (14/15)\n",
      "blocksworld:  32%|█████████▌                    | 15/47 [26:19<49:39, 93.12s/it]\n",
      "MCTS iteration:   0%|                                    | 0/10 [00:00<?, ?it/s]\u001b[A/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "\n",
      "MCTS iteration:  10%|██▊                         | 1/10 [01:31<13:46, 91.87s/it]\u001b[A/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "\n",
      "MCTS iteration:  20%|█████▌                      | 2/10 [03:01<12:04, 90.51s/it]\u001b[A\n",
      "                                                                                \u001b[A[+]: Saving plan in tmp_plan.txt\n",
      "RESPONSE::: Checking plan: tmp_plan.txt\n",
      "Plan executed successfully - checking goal\n",
      "Plan valid\n",
      "Final value: 2 \n",
      "\n",
      "Successful plans:\n",
      "Value: 2\n",
      " tmp_plan.txt 2 \n",
      "\n",
      "\n",
      "Case #16: correct=True, output='unstack the orange block from on top of the yellow block\\nstack the orange block on top of the red block', answer={'init': 'the red block is clear, the orange block is clear, the hand is empty, the red block is on top of the blue block, the orange block is on top of the yellow block, the blue block is on the table and the yellow block is on the table', 'goal': 'the orange block is on top of the red block', 'plan': '\\nunstack the orange block from on top of the yellow block\\nstack the orange block on top of the red block\\n[PLAN END]\\n', 'question': '\\n[STATEMENT]\\nAs initial conditions I have that, the red block is clear, the orange block is clear, the hand is empty, the red block is on top of the blue block, the orange block is on top of the yellow block, the blue block is on the table and the yellow block is on the table.\\nMy goal is to have that the orange block is on top of the red block.\\n\\nMy plan is as follows:\\n\\n[PLAN]\\n', 'instance_file': 'LLMs-Planning/llm_planning_analysis/instances/blocksworld/generated_basic/instance-430.pddl'};accuracy=0.938 (15/16)\n",
      "blocksworld:  34%|█████████▏                 | 16/47 [29:23<1:02:06, 120.21s/it]\n",
      "MCTS iteration:   0%|                                    | 0/10 [00:00<?, ?it/s]\u001b[A/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "\n",
      "MCTS iteration:  10%|██▊                         | 1/10 [01:31<13:46, 91.89s/it]\u001b[A\n",
      "                                                                                \u001b[A[+]: Saving plan in tmp_plan.txt\n",
      "RESPONSE::: Checking plan: tmp_plan.txt\n",
      "Plan executed successfully - checking goal\n",
      "Plan valid\n",
      "Final value: 2 \n",
      "\n",
      "Successful plans:\n",
      "Value: 2\n",
      " tmp_plan.txt 2 \n",
      "\n",
      "\n",
      "Case #17: correct=True, output='unstack the blue block from on top of the yellow block\\nstack the blue block on top of the orange block', answer={'init': 'the blue block is clear, the orange block is clear, the hand is empty, the blue block is on top of the yellow block, the orange block is on top of the red block, the red block is on the table and the yellow block is on the table', 'goal': 'the blue block is on top of the orange block', 'plan': '\\nunstack the blue block from on top of the yellow block\\nstack the blue block on top of the orange block\\n[PLAN END]\\n', 'question': '\\n[STATEMENT]\\nAs initial conditions I have that, the blue block is clear, the orange block is clear, the hand is empty, the blue block is on top of the yellow block, the orange block is on top of the red block, the red block is on the table and the yellow block is on the table.\\nMy goal is to have that the blue block is on top of the orange block.\\n\\nMy plan is as follows:\\n\\n[PLAN]\\n', 'instance_file': 'LLMs-Planning/llm_planning_analysis/instances/blocksworld/generated_basic/instance-46.pddl'};accuracy=0.941 (16/17)\n",
      "blocksworld:  36%|██████████▍                  | 17/47 [30:56<56:05, 112.17s/it]\n",
      "MCTS iteration:   0%|                                    | 0/10 [00:00<?, ?it/s]\u001b[A/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Error: not recognized\n",
      "i am holding the yellow block and the blue block is now clear\n",
      "\n",
      "MCTS iteration:  10%|██▊                         | 1/10 [01:29<13:28, 89.82s/it]\u001b[A\n",
      "                                                                                \u001b[A[+]: Saving plan in tmp_plan.txt\n",
      "RESPONSE::: Checking plan: tmp_plan.txt\n",
      "Plan executed successfully - checking goal\n",
      "Goal not satisfied\n",
      "Plan invalid\n",
      "\n",
      "\n",
      "Failed plans:\n",
      " tmp_plan.txt \n",
      "\n",
      "Case #18: correct=False, output='pick up the yellow block\\nstack the yellow block on top of the blue block', answer={'init': 'the blue block is clear, the yellow block is clear, the hand is empty, the blue block is on top of the orange block, the orange block is on top of the red block, the red block is on the table and the yellow block is on the table', 'goal': 'the blue block is on top of the yellow block and the orange block is on top of the red block', 'plan': '\\nunstack the blue block from on top of the orange block\\nstack the blue block on top of the yellow block\\n[PLAN END]\\n', 'question': '\\n[STATEMENT]\\nAs initial conditions I have that, the blue block is clear, the yellow block is clear, the hand is empty, the blue block is on top of the orange block, the orange block is on top of the red block, the red block is on the table and the yellow block is on the table.\\nMy goal is to have that the blue block is on top of the yellow block and the orange block is on top of the red block.\\n\\nMy plan is as follows:\\n\\n[PLAN]\\n', 'instance_file': 'LLMs-Planning/llm_planning_analysis/instances/blocksworld/generated_basic/instance-41.pddl'};accuracy=0.889 (16/18)\n",
      "blocksworld:  38%|███████████                  | 18/47 [32:27<51:12, 105.94s/it]\n",
      "MCTS iteration:   0%|                                    | 0/10 [00:00<?, ?it/s]\u001b[A/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Error: zero-colors\n",
      "\n",
      "\n",
      "MCTS iteration:  10%|██▊                         | 1/10 [01:29<13:27, 89.75s/it]\u001b[A\n",
      "                                                                                \u001b[A[+]: Saving plan in tmp_plan.txt\n",
      "RESPONSE::: Checking plan: tmp_plan.txt\n",
      "Plan executed successfully - checking goal\n",
      "Plan valid\n",
      "Final value: 2 \n",
      "\n",
      "Successful plans:\n",
      "Value: 2\n",
      " tmp_plan.txt 2 \n",
      "\n",
      "\n",
      "Case #19: correct=True, output='pick up the yellow block\\nstack the yellow block on top of the orange block', answer={'init': 'the orange block is clear, the yellow block is clear, the hand is empty, the blue block is on top of the red block, the orange block is on top of the blue block, the red block is on the table and the yellow block is on the table', 'goal': 'the blue block is on top of the red block and the yellow block is on top of the orange block', 'plan': '\\npick up the yellow block\\nstack the yellow block on top of the orange block\\n[PLAN END]\\n', 'question': '\\n[STATEMENT]\\nAs initial conditions I have that, the orange block is clear, the yellow block is clear, the hand is empty, the blue block is on top of the red block, the orange block is on top of the blue block, the red block is on the table and the yellow block is on the table.\\nMy goal is to have that the blue block is on top of the red block and the yellow block is on top of the orange block.\\n\\nMy plan is as follows:\\n\\n[PLAN]\\n', 'instance_file': 'LLMs-Planning/llm_planning_analysis/instances/blocksworld/generated_basic/instance-5.pddl'};accuracy=0.895 (17/19)\n",
      "blocksworld:  40%|███████████▋                 | 19/47 [33:59<47:23, 101.56s/it]\n",
      "MCTS iteration:   0%|                                    | 0/10 [00:00<?, ?it/s]\u001b[A/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "\n",
      "MCTS iteration:  10%|██▊                         | 1/10 [01:29<13:26, 89.57s/it]\u001b[A\n",
      "                                                                                \u001b[A[+]: Saving plan in tmp_plan.txt\n",
      "RESPONSE::: Checking plan: tmp_plan.txt\n",
      "Plan executed successfully - checking goal\n",
      "Goal not satisfied\n",
      "Plan invalid\n",
      "\n",
      "\n",
      "Failed plans:\n",
      " tmp_plan.txt \n",
      "\n",
      "Case #20: correct=False, output='pick up the orange block\\nstack the orange block on top of the red block', answer={'init': 'the red block is clear, the orange block is clear, the hand is empty, the red block is on top of the yellow block, the yellow block is on top of the blue block, the blue block is on the table and the orange block is on the table', 'goal': 'the red block is on top of the orange block', 'plan': '\\nunstack the red block from on top of the yellow block\\nstack the red block on top of the orange block\\n[PLAN END]\\n', 'question': '\\n[STATEMENT]\\nAs initial conditions I have that, the red block is clear, the orange block is clear, the hand is empty, the red block is on top of the yellow block, the yellow block is on top of the blue block, the blue block is on the table and the orange block is on the table.\\nMy goal is to have that the red block is on top of the orange block.\\n\\nMy plan is as follows:\\n\\n[PLAN]\\n', 'instance_file': 'LLMs-Planning/llm_planning_analysis/instances/blocksworld/generated_basic/instance-123.pddl'};accuracy=0.850 (17/20)\n",
      "blocksworld:  43%|████████████▊                 | 20/47 [35:30<44:17, 98.44s/it]\n",
      "MCTS iteration:   0%|                                    | 0/10 [00:00<?, ?it/s]\u001b[A/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "\n",
      "MCTS iteration:  10%|██▊                         | 1/10 [01:31<13:46, 91.81s/it]\u001b[A/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "\n",
      "MCTS iteration:  30%|████████▍                   | 3/10 [03:00<06:37, 56.82s/it]\u001b[A\n",
      "                                                                                \u001b[A[+]: Saving plan in tmp_plan.txt\n",
      "RESPONSE::: Checking plan: tmp_plan.txt\n",
      "Plan executed successfully - checking goal\n",
      "Plan valid\n",
      "Final value: 2 \n",
      "\n",
      "Successful plans:\n",
      "Value: 2\n",
      " tmp_plan.txt 2 \n",
      "\n",
      "\n",
      "Case #21: correct=True, output='unstack the orange block from on top of the yellow block\\nstack the orange block on top of the red block', answer={'init': 'the red block is clear, the orange block is clear, the hand is empty, the red block is on top of the blue block, the orange block is on top of the yellow block, the blue block is on the table and the yellow block is on the table', 'goal': 'the red block is on top of the blue block and the orange block is on top of the red block', 'plan': '\\nunstack the orange block from on top of the yellow block\\nstack the orange block on top of the red block\\n[PLAN END]\\n', 'question': '\\n[STATEMENT]\\nAs initial conditions I have that, the red block is clear, the orange block is clear, the hand is empty, the red block is on top of the blue block, the orange block is on top of the yellow block, the blue block is on the table and the yellow block is on the table.\\nMy goal is to have that the red block is on top of the blue block and the orange block is on top of the red block.\\n\\nMy plan is as follows:\\n\\n[PLAN]\\n', 'instance_file': 'LLMs-Planning/llm_planning_analysis/instances/blocksworld/generated_basic/instance-252.pddl'};accuracy=0.857 (18/21)\n",
      "blocksworld:  45%|████████████▉                | 21/47 [38:33<53:36, 123.71s/it]\n",
      "MCTS iteration:   0%|                                    | 0/10 [00:00<?, ?it/s]\u001b[A/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "\n",
      "MCTS iteration:  10%|██▊                         | 1/10 [01:31<13:46, 91.79s/it]\u001b[A\n",
      "                                                                                \u001b[A[+]: Saving plan in tmp_plan.txt\n",
      "RESPONSE::: Checking plan: tmp_plan.txt\n",
      "Plan executed successfully - checking goal\n",
      "Plan valid\n",
      "Final value: 2 \n",
      "\n",
      "Successful plans:\n",
      "Value: 2\n",
      " tmp_plan.txt 2 \n",
      "\n",
      "\n",
      "Case #22: correct=True, output='unstack the red block from on top of the orange block\\nstack the red block on top of the blue block', answer={'init': 'the red block is clear, the blue block is clear, the hand is empty, the red block is on top of the orange block, the blue block is on top of the yellow block, the orange block is on the table and the yellow block is on the table', 'goal': 'the red block is on top of the blue block', 'plan': '\\nunstack the red block from on top of the orange block\\nstack the red block on top of the blue block\\n[PLAN END]\\n', 'question': '\\n[STATEMENT]\\nAs initial conditions I have that, the red block is clear, the blue block is clear, the hand is empty, the red block is on top of the orange block, the blue block is on top of the yellow block, the orange block is on the table and the yellow block is on the table.\\nMy goal is to have that the red block is on top of the blue block.\\n\\nMy plan is as follows:\\n\\n[PLAN]\\n', 'instance_file': 'LLMs-Planning/llm_planning_analysis/instances/blocksworld/generated_basic/instance-362.pddl'};accuracy=0.864 (19/22)\n",
      "blocksworld:  47%|█████████████▌               | 22/47 [40:06<47:45, 114.61s/it]\n",
      "MCTS iteration:   0%|                                    | 0/10 [00:00<?, ?it/s]\u001b[A/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "\n",
      "MCTS iteration:  10%|██▊                         | 1/10 [01:32<13:52, 92.50s/it]\u001b[A\n",
      "                                                                                \u001b[A[+]: Saving plan in tmp_plan.txt\n",
      "RESPONSE::: Checking plan: tmp_plan.txt\n",
      "Plan executed successfully - checking goal\n",
      "Plan valid\n",
      "Final value: 2 \n",
      "\n",
      "Successful plans:\n",
      "Value: 2\n",
      " tmp_plan.txt 2 \n",
      "\n",
      "\n",
      "Case #23: correct=True, output='pick up the orange block\\nstack the orange block on top of the red block', answer={'init': 'the red block is clear, the blue block is clear, the orange block is clear, the hand is empty, the blue block is on top of the yellow block, the red block is on the table, the orange block is on the table and the yellow block is on the table', 'goal': 'the orange block is on top of the red block', 'plan': '\\npick up the orange block\\nstack the orange block on top of the red block\\n[PLAN END]\\n', 'question': '\\n[STATEMENT]\\nAs initial conditions I have that, the red block is clear, the blue block is clear, the orange block is clear, the hand is empty, the blue block is on top of the yellow block, the red block is on the table, the orange block is on the table and the yellow block is on the table.\\nMy goal is to have that the orange block is on top of the red block.\\n\\nMy plan is as follows:\\n\\n[PLAN]\\n', 'instance_file': 'LLMs-Planning/llm_planning_analysis/instances/blocksworld/generated_basic/instance-164.pddl'};accuracy=0.870 (20/23)\n",
      "blocksworld:  49%|██████████████▏              | 23/47 [41:40<43:23, 108.48s/it]\n",
      "MCTS iteration:   0%|                                    | 0/10 [00:00<?, ?it/s]\u001b[A/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Error: not recognized\n",
      "i am holding the yellow block\n",
      "/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Error: zero-colors\n",
      "i have that\n",
      "\n",
      "MCTS iteration:  10%|██▊                         | 1/10 [01:32<13:50, 92.33s/it]\u001b[A\n",
      "                                                                                \u001b[A[+]: Saving plan in tmp_plan.txt\n",
      "RESPONSE::: Checking plan: tmp_plan.txt\n",
      "Plan executed successfully - checking goal\n",
      "Plan valid\n",
      "Final value: 2 \n",
      "\n",
      "Successful plans:\n",
      "Value: 2\n",
      " tmp_plan.txt 2 \n",
      "\n",
      "\n",
      "Case #24: correct=True, output='pick up the yellow block\\nstack the yellow block on top of the blue block', answer={'init': 'the red block is clear, the blue block is clear, the yellow block is clear, the hand is empty, the red block is on top of the orange block, the blue block is on the table, the orange block is on the table and the yellow block is on the table', 'goal': 'the yellow block is on top of the blue block', 'plan': '\\npick up the yellow block\\nstack the yellow block on top of the blue block\\n[PLAN END]\\n', 'question': '\\n[STATEMENT]\\nAs initial conditions I have that, the red block is clear, the blue block is clear, the yellow block is clear, the hand is empty, the red block is on top of the orange block, the blue block is on the table, the orange block is on the table and the yellow block is on the table.\\nMy goal is to have that the yellow block is on top of the blue block.\\n\\nMy plan is as follows:\\n\\n[PLAN]\\n', 'instance_file': 'LLMs-Planning/llm_planning_analysis/instances/blocksworld/generated_basic/instance-312.pddl'};accuracy=0.875 (21/24)\n",
      "blocksworld:  51%|██████████████▊              | 24/47 [43:14<39:54, 104.12s/it]\n",
      "MCTS iteration:   0%|                                    | 0/10 [00:00<?, ?it/s]\u001b[A/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Error: zero-colors\n",
      "\n",
      "/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "\n",
      "MCTS iteration:  10%|██▊                         | 1/10 [01:32<13:52, 92.54s/it]\u001b[A/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Error: not recognized\n",
      "i am holding the blue block\n",
      "\n",
      "MCTS iteration:  20%|█████▌                      | 2/10 [03:02<12:07, 90.97s/it]\u001b[A\n",
      "                                                                                \u001b[A[+]: Saving plan in tmp_plan.txt\n",
      "RESPONSE::: Checking plan: tmp_plan.txt\n",
      "Plan executed successfully - checking goal\n",
      "Plan valid\n",
      "Final value: 2 \n",
      "\n",
      "Successful plans:\n",
      "Value: 2\n",
      " tmp_plan.txt 2 \n",
      "\n",
      "\n",
      "Case #25: correct=True, output='unstack the blue block from on top of the yellow block\\nstack the blue block on top of the red block', answer={'init': 'the red block is clear, the blue block is clear, the hand is empty, the red block is on top of the orange block, the blue block is on top of the yellow block, the orange block is on the table and the yellow block is on the table', 'goal': 'the red block is on top of the orange block and the blue block is on top of the red block', 'plan': '\\nunstack the blue block from on top of the yellow block\\nstack the blue block on top of the red block\\n[PLAN END]\\n', 'question': '\\n[STATEMENT]\\nAs initial conditions I have that, the red block is clear, the blue block is clear, the hand is empty, the red block is on top of the orange block, the blue block is on top of the yellow block, the orange block is on the table and the yellow block is on the table.\\nMy goal is to have that the red block is on top of the orange block and the blue block is on top of the red block.\\n\\nMy plan is as follows:\\n\\n[PLAN]\\n', 'instance_file': 'LLMs-Planning/llm_planning_analysis/instances/blocksworld/generated_basic/instance-31.pddl'};accuracy=0.880 (22/25)\n",
      "blocksworld:  53%|███████████████▍             | 25/47 [46:18<46:58, 128.09s/it]\n",
      "MCTS iteration:   0%|                                    | 0/10 [00:00<?, ?it/s]\u001b[A/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "\n",
      "MCTS iteration:  10%|██▊                         | 1/10 [01:29<13:28, 89.87s/it]\u001b[A\n",
      "                                                                                \u001b[A[+]: Saving plan in tmp_plan.txt\n",
      "RESPONSE::: Checking plan: tmp_plan.txt\n",
      "Plan executed successfully - checking goal\n",
      "Plan valid\n",
      "Final value: 2 \n",
      "\n",
      "Successful plans:\n",
      "Value: 2\n",
      " tmp_plan.txt 2 \n",
      "\n",
      "\n",
      "Case #26: correct=True, output='pick up the blue block\\nstack the blue block on top of the yellow block', answer={'init': 'the blue block is clear, the yellow block is clear, the hand is empty, the red block is on top of the orange block, the yellow block is on top of the red block, the blue block is on the table and the orange block is on the table', 'goal': 'the red block is on top of the orange block, the blue block is on top of the yellow block and the yellow block is on top of the red block', 'plan': '\\npick up the blue block\\nstack the blue block on top of the yellow block\\n[PLAN END]\\n', 'question': '\\n[STATEMENT]\\nAs initial conditions I have that, the blue block is clear, the yellow block is clear, the hand is empty, the red block is on top of the orange block, the yellow block is on top of the red block, the blue block is on the table and the orange block is on the table.\\nMy goal is to have that the red block is on top of the orange block, the blue block is on top of the yellow block and the yellow block is on top of the red block.\\n\\nMy plan is as follows:\\n\\n[PLAN]\\n', 'instance_file': 'LLMs-Planning/llm_planning_analysis/instances/blocksworld/generated_basic/instance-21.pddl'};accuracy=0.885 (23/26)\n",
      "blocksworld:  55%|████████████████             | 26/47 [47:50<40:59, 117.10s/it]\n",
      "MCTS iteration:   0%|                                    | 0/10 [00:00<?, ?it/s]\u001b[A/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Error: zero-colors\n",
      "i can't answer this\n",
      "\n",
      "MCTS iteration:  10%|██▊                         | 1/10 [01:35<14:16, 95.13s/it]\u001b[A\n",
      "                                                                                \u001b[A[+]: Saving plan in tmp_plan.txt\n",
      "RESPONSE::: Checking plan: tmp_plan.txt\n",
      "Plan executed successfully - checking goal\n",
      "Plan valid\n",
      "Final value: 2 \n",
      "\n",
      "Successful plans:\n",
      "Value: 2\n",
      " tmp_plan.txt 2 \n",
      "\n",
      "\n",
      "Case #27: correct=True, output='pick up the red block\\nstack the red block on top of the yellow block', answer={'init': 'the red block is clear, the yellow block is clear, the hand is empty, the orange block is on top of the blue block, the yellow block is on top of the orange block, the red block is on the table and the blue block is on the table', 'goal': 'the red block is on top of the yellow block, the orange block is on top of the blue block and the yellow block is on top of the orange block', 'plan': '\\npick up the red block\\nstack the red block on top of the yellow block\\n[PLAN END]\\n', 'question': '\\n[STATEMENT]\\nAs initial conditions I have that, the red block is clear, the yellow block is clear, the hand is empty, the orange block is on top of the blue block, the yellow block is on top of the orange block, the red block is on the table and the blue block is on the table.\\nMy goal is to have that the red block is on top of the yellow block, the orange block is on top of the blue block and the yellow block is on top of the orange block.\\n\\nMy plan is as follows:\\n\\n[PLAN]\\n', 'instance_file': 'LLMs-Planning/llm_planning_analysis/instances/blocksworld/generated_basic/instance-142.pddl'};accuracy=0.889 (24/27)\n",
      "blocksworld:  57%|████████████████▋            | 27/47 [49:26<36:59, 110.99s/it]\n",
      "MCTS iteration:   0%|                                    | 0/10 [00:00<?, ?it/s]\u001b[A/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Error: zero-colors\n",
      "i have that\n",
      "/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Error: zero-colors\n",
      "i have that\n",
      "\n",
      "MCTS iteration:  10%|██▊                         | 1/10 [01:32<13:51, 92.40s/it]\u001b[A\n",
      "                                                                                \u001b[A[+]: Saving plan in tmp_plan.txt\n",
      "RESPONSE::: Checking plan: tmp_plan.txt\n",
      "Plan executed successfully - checking goal\n",
      "Plan valid\n",
      "Final value: 2 \n",
      "\n",
      "Successful plans:\n",
      "Value: 2\n",
      " tmp_plan.txt 2 \n",
      "\n",
      "\n",
      "Case #28: correct=True, output='pick up the yellow block\\nstack the yellow block on top of the blue block', answer={'init': 'the blue block is clear, the orange block is clear, the yellow block is clear, the hand is empty, the blue block is on top of the red block, the red block is on the table, the orange block is on the table and the yellow block is on the table', 'goal': 'the yellow block is on top of the blue block', 'plan': '\\npick up the yellow block\\nstack the yellow block on top of the blue block\\n[PLAN END]\\n', 'question': '\\n[STATEMENT]\\nAs initial conditions I have that, the blue block is clear, the orange block is clear, the yellow block is clear, the hand is empty, the blue block is on top of the red block, the red block is on the table, the orange block is on the table and the yellow block is on the table.\\nMy goal is to have that the yellow block is on top of the blue block.\\n\\nMy plan is as follows:\\n\\n[PLAN]\\n', 'instance_file': 'LLMs-Planning/llm_planning_analysis/instances/blocksworld/generated_basic/instance-170.pddl'};accuracy=0.893 (25/28)\n",
      "blocksworld:  60%|█████████████████▎           | 28/47 [51:00<33:32, 105.94s/it]\n",
      "MCTS iteration:   0%|                                    | 0/10 [00:00<?, ?it/s]\u001b[A/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "\n",
      "MCTS iteration:  10%|██▊                         | 1/10 [01:29<13:28, 89.87s/it]\u001b[A\n",
      "                                                                                \u001b[A[+]: Saving plan in tmp_plan.txt\n",
      "RESPONSE::: Checking plan: tmp_plan.txt\n",
      "Plan executed successfully - checking goal\n",
      "Goal not satisfied\n",
      "Plan invalid\n",
      "\n",
      "\n",
      "Failed plans:\n",
      " tmp_plan.txt \n",
      "\n",
      "Case #29: correct=False, output='pick up the yellow block\\nstack the yellow block on top of the orange block', answer={'init': 'the orange block is clear, the yellow block is clear, the hand is empty, the blue block is on top of the red block, the orange block is on top of the blue block, the red block is on the table and the yellow block is on the table', 'goal': 'the orange block is on top of the yellow block', 'plan': '\\nunstack the orange block from on top of the blue block\\nstack the orange block on top of the yellow block\\n[PLAN END]\\n', 'question': '\\n[STATEMENT]\\nAs initial conditions I have that, the orange block is clear, the yellow block is clear, the hand is empty, the blue block is on top of the red block, the orange block is on top of the blue block, the red block is on the table and the yellow block is on the table.\\nMy goal is to have that the orange block is on top of the yellow block.\\n\\nMy plan is as follows:\\n\\n[PLAN]\\n', 'instance_file': 'LLMs-Planning/llm_planning_analysis/instances/blocksworld/generated_basic/instance-70.pddl'};accuracy=0.862 (25/29)\n",
      "blocksworld:  62%|█████████████████▉           | 29/47 [52:32<30:28, 101.58s/it]\n",
      "MCTS iteration:   0%|                                    | 0/10 [00:00<?, ?it/s]\u001b[A/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "\n",
      "MCTS iteration:  10%|██▊                         | 1/10 [01:30<13:31, 90.19s/it]\u001b[A\n",
      "                                                                                \u001b[A[+]: Saving plan in tmp_plan.txt\n",
      "RESPONSE::: Checking plan: tmp_plan.txt\n",
      "Plan executed successfully - checking goal\n",
      "Plan valid\n",
      "Final value: 2 \n",
      "\n",
      "Successful plans:\n",
      "Value: 2\n",
      " tmp_plan.txt 2 \n",
      "\n",
      "\n",
      "Case #30: correct=True, output='pick up the orange block\\nstack the orange block on top of the yellow block', answer={'init': 'the orange block is clear, the yellow block is clear, the hand is empty, the red block is on top of the blue block, the yellow block is on top of the red block, the blue block is on the table and the orange block is on the table', 'goal': 'the red block is on top of the blue block and the orange block is on top of the yellow block', 'plan': '\\npick up the orange block\\nstack the orange block on top of the yellow block\\n[PLAN END]\\n', 'question': '\\n[STATEMENT]\\nAs initial conditions I have that, the orange block is clear, the yellow block is clear, the hand is empty, the red block is on top of the blue block, the yellow block is on top of the red block, the blue block is on the table and the orange block is on the table.\\nMy goal is to have that the red block is on top of the blue block and the orange block is on top of the yellow block.\\n\\nMy plan is as follows:\\n\\n[PLAN]\\n', 'instance_file': 'LLMs-Planning/llm_planning_analysis/instances/blocksworld/generated_basic/instance-233.pddl'};accuracy=0.867 (26/30)\n",
      "blocksworld:  64%|███████████████████▏          | 30/47 [54:04<27:56, 98.62s/it]\n",
      "MCTS iteration:   0%|                                    | 0/10 [00:00<?, ?it/s]\u001b[A/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Error: zero-colors\n",
      "i have that\n",
      "/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "\n",
      "MCTS iteration:  10%|██▊                         | 1/10 [01:29<13:29, 89.95s/it]\u001b[A\n",
      "                                                                                \u001b[A[+]: Saving plan in tmp_plan.txt\n",
      "RESPONSE::: Checking plan: tmp_plan.txt\n",
      "Plan executed successfully - checking goal\n",
      "Plan valid\n",
      "Final value: 2 \n",
      "\n",
      "Successful plans:\n",
      "Value: 2\n",
      " tmp_plan.txt 2 \n",
      "\n",
      "\n",
      "Case #31: correct=True, output='pick up the blue block\\nstack the blue block on top of the red block', answer={'init': 'the red block is clear, the blue block is clear, the hand is empty, the red block is on top of the orange block, the blue block is on the table and the orange block is on the table', 'goal': 'the blue block is on top of the red block', 'plan': '\\npick up the blue block\\nstack the blue block on top of the red block\\n[PLAN END]\\n', 'question': '\\n[STATEMENT]\\nAs initial conditions I have that, the red block is clear, the blue block is clear, the hand is empty, the red block is on top of the orange block, the blue block is on the table and the orange block is on the table.\\nMy goal is to have that the blue block is on top of the red block.\\n\\nMy plan is as follows:\\n\\n[PLAN]\\n', 'instance_file': 'LLMs-Planning/llm_planning_analysis/instances/blocksworld/generated_basic_3/instance-20.pddl'};accuracy=0.871 (27/31)\n",
      "blocksworld:  66%|███████████████████▊          | 31/47 [55:35<25:44, 96.50s/it]\n",
      "MCTS iteration:   0%|                                    | 0/10 [00:00<?, ?it/s]\u001b[A/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Error: zero-colors\n",
      "i can only pick up or unstack a block if my hand is empty. \n",
      "i can only pick up a block if the block is on the table and the block is clear. a block is clear if the block has no other blocks on top of it and if the block is not picked up. \n",
      "i can only pick up a block if the block is on the table and the block is clear. once i pick up a block\n",
      "/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "\n",
      "MCTS iteration:  10%|██▊                         | 1/10 [01:29<13:29, 89.95s/it]\u001b[A\n",
      "                                                                                \u001b[A[+]: Saving plan in tmp_plan.txt\n",
      "RESPONSE::: Checking plan: tmp_plan.txt\n",
      "Plan executed successfully - checking goal\n",
      "Plan valid\n",
      "Final value: 2 \n",
      "\n",
      "Successful plans:\n",
      "Value: 2\n",
      " tmp_plan.txt 2 \n",
      "\n",
      "\n",
      "Case #32: correct=True, output='pick up the blue block\\nstack the blue block on top of the orange block', answer={'init': 'the blue block is clear, the orange block is clear, the hand is empty, the orange block is on top of the red block, the red block is on the table and the blue block is on the table', 'goal': 'the blue block is on top of the orange block', 'plan': '\\npick up the blue block\\nstack the blue block on top of the orange block\\n[PLAN END]\\n', 'question': '\\n[STATEMENT]\\nAs initial conditions I have that, the blue block is clear, the orange block is clear, the hand is empty, the orange block is on top of the red block, the red block is on the table and the blue block is on the table.\\nMy goal is to have that the blue block is on top of the orange block.\\n\\nMy plan is as follows:\\n\\n[PLAN]\\n', 'instance_file': 'LLMs-Planning/llm_planning_analysis/instances/blocksworld/generated_basic_3/instance-73.pddl'};accuracy=0.875 (28/32)\n",
      "blocksworld:  68%|████████████████████▍         | 32/47 [57:07<23:45, 95.02s/it]\n",
      "MCTS iteration:   0%|                                    | 0/10 [00:00<?, ?it/s]\u001b[A/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "\n",
      "MCTS iteration:  10%|██▊                         | 1/10 [01:30<13:31, 90.20s/it]\u001b[A\n",
      "                                                                                \u001b[A[+]: Saving plan in tmp_plan.txt\n",
      "RESPONSE::: Checking plan: tmp_plan.txt\n",
      "Plan executed successfully - checking goal\n",
      "Plan valid\n",
      "Final value: 2 \n",
      "\n",
      "Successful plans:\n",
      "Value: 2\n",
      " tmp_plan.txt 2 \n",
      "\n",
      "\n",
      "Case #33: correct=True, output='pick up the blue block\\nstack the blue block on top of the orange block', answer={'init': 'the blue block is clear, the orange block is clear, the hand is empty, the orange block is on top of the red block, the red block is on the table and the blue block is on the table', 'goal': 'the blue block is on top of the orange block and the orange block is on top of the red block', 'plan': '\\npick up the blue block\\nstack the blue block on top of the orange block\\n[PLAN END]\\n', 'question': '\\n[STATEMENT]\\nAs initial conditions I have that, the blue block is clear, the orange block is clear, the hand is empty, the orange block is on top of the red block, the red block is on the table and the blue block is on the table.\\nMy goal is to have that the blue block is on top of the orange block and the orange block is on top of the red block.\\n\\nMy plan is as follows:\\n\\n[PLAN]\\n', 'instance_file': 'LLMs-Planning/llm_planning_analysis/instances/blocksworld/generated_basic_3/instance-100.pddl'};accuracy=0.879 (29/33)\n",
      "blocksworld:  70%|█████████████████████         | 33/47 [58:38<21:56, 94.04s/it]\n",
      "MCTS iteration:   0%|                                    | 0/10 [00:00<?, ?it/s]\u001b[A/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "\n",
      "MCTS iteration:  10%|██▊                         | 1/10 [01:32<13:51, 92.35s/it]\u001b[A\n",
      "                                                                                \u001b[A[+]: Saving plan in tmp_plan.txt\n",
      "RESPONSE::: Checking plan: tmp_plan.txt\n",
      "Plan executed successfully - checking goal\n",
      "Plan valid\n",
      "Final value: 2 \n",
      "\n",
      "Successful plans:\n",
      "Value: 2\n",
      " tmp_plan.txt 2 \n",
      "\n",
      "\n",
      "Case #34: correct=True, output='unstack the orange block from on top of the red block\\nstack the orange block on top of the blue block', answer={'init': 'the blue block is clear, the orange block is clear, the hand is empty, the orange block is on top of the red block, the red block is on the table and the blue block is on the table', 'goal': 'the orange block is on top of the blue block', 'plan': '\\nunstack the orange block from on top of the red block\\nstack the orange block on top of the blue block\\n[PLAN END]\\n', 'question': '\\n[STATEMENT]\\nAs initial conditions I have that, the blue block is clear, the orange block is clear, the hand is empty, the orange block is on top of the red block, the red block is on the table and the blue block is on the table.\\nMy goal is to have that the orange block is on top of the blue block.\\n\\nMy plan is as follows:\\n\\n[PLAN]\\n', 'instance_file': 'LLMs-Planning/llm_planning_analysis/instances/blocksworld/generated_basic_3/instance-36.pddl'};accuracy=0.882 (30/34)\n",
      "blocksworld:  72%|████████████████████▎       | 34/47 [1:00:12<20:22, 94.02s/it]\n",
      "MCTS iteration:   0%|                                    | 0/10 [00:00<?, ?it/s]\u001b[A/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Error: zero-colors\n",
      "\n",
      "\n",
      "MCTS iteration:  10%|██▊                         | 1/10 [01:32<13:50, 92.26s/it]\u001b[A\n",
      "                                                                                \u001b[A[+]: Saving plan in tmp_plan.txt\n",
      "RESPONSE::: Checking plan: tmp_plan.txt\n",
      "Plan executed successfully - checking goal\n",
      "Plan valid\n",
      "Final value: 2 \n",
      "\n",
      "Successful plans:\n",
      "Value: 2\n",
      " tmp_plan.txt 2 \n",
      "\n",
      "\n",
      "Case #35: correct=True, output='pick up the red block\\nstack the red block on top of the orange block', answer={'init': 'the red block is clear, the blue block is clear, the orange block is clear, the hand is empty, the red block is on the table, the blue block is on the table and the orange block is on the table', 'goal': 'the red block is on top of the orange block', 'plan': '\\npick up the red block\\nstack the red block on top of the orange block\\n[PLAN END]\\n', 'question': '\\n[STATEMENT]\\nAs initial conditions I have that, the red block is clear, the blue block is clear, the orange block is clear, the hand is empty, the red block is on the table, the blue block is on the table and the orange block is on the table.\\nMy goal is to have that the red block is on top of the orange block.\\n\\nMy plan is as follows:\\n\\n[PLAN]\\n', 'instance_file': 'LLMs-Planning/llm_planning_analysis/instances/blocksworld/generated_basic_3/instance-8.pddl'};accuracy=0.886 (31/35)\n",
      "blocksworld:  74%|████████████████████▊       | 35/47 [1:01:46<18:47, 93.96s/it]\n",
      "MCTS iteration:   0%|                                    | 0/10 [00:00<?, ?it/s]\u001b[A/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Error: no successful change\n",
      "the orange block is in the hand\n",
      "['the red block is clear', 'the blue block is clear', 'the orange block is clear', 'the hand is empty', 'the red block is on the table', 'the blue block is on the table', 'the orange block is on the table']\n",
      "/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "\n",
      "MCTS iteration:  10%|██▊                         | 1/10 [01:32<13:53, 92.56s/it]\u001b[A\n",
      "                                                                                \u001b[A[+]: Saving plan in tmp_plan.txt\n",
      "RESPONSE::: Checking plan: tmp_plan.txt\n",
      "Plan executed successfully - checking goal\n",
      "Plan valid\n",
      "Final value: 2 \n",
      "\n",
      "Successful plans:\n",
      "Value: 2\n",
      " tmp_plan.txt 2 \n",
      "\n",
      "\n",
      "Case #36: correct=True, output='pick up the orange block\\nstack the orange block on top of the red block', answer={'init': 'the red block is clear, the blue block is clear, the orange block is clear, the hand is empty, the red block is on the table, the blue block is on the table and the orange block is on the table', 'goal': 'the orange block is on top of the red block', 'plan': '\\npick up the orange block\\nstack the orange block on top of the red block\\n[PLAN END]\\n', 'question': '\\n[STATEMENT]\\nAs initial conditions I have that, the red block is clear, the blue block is clear, the orange block is clear, the hand is empty, the red block is on the table, the blue block is on the table and the orange block is on the table.\\nMy goal is to have that the orange block is on top of the red block.\\n\\nMy plan is as follows:\\n\\n[PLAN]\\n', 'instance_file': 'LLMs-Planning/llm_planning_analysis/instances/blocksworld/generated_basic_3/instance-93.pddl'};accuracy=0.889 (32/36)\n",
      "blocksworld:  77%|█████████████████████▍      | 36/47 [1:03:21<17:14, 94.04s/it]\n",
      "MCTS iteration:   0%|                                    | 0/10 [00:00<?, ?it/s]\u001b[A/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "\n",
      "MCTS iteration:  10%|██▊                         | 1/10 [01:33<13:59, 93.24s/it]\u001b[A\n",
      "                                                                                \u001b[A[+]: Saving plan in tmp_plan.txt\n",
      "RESPONSE::: Checking plan: tmp_plan.txt\n",
      "Plan executed successfully - checking goal\n",
      "Plan valid\n",
      "Final value: 2 \n",
      "\n",
      "Successful plans:\n",
      "Value: 2\n",
      " tmp_plan.txt 2 \n",
      "\n",
      "\n",
      "Case #37: correct=True, output='pick up the red block\\nstack the red block on top of the orange block', answer={'init': 'the red block is clear, the orange block is clear, the hand is empty, the orange block is on top of the blue block, the red block is on the table and the blue block is on the table', 'goal': 'the red block is on top of the orange block', 'plan': '\\npick up the red block\\nstack the red block on top of the orange block\\n[PLAN END]\\n', 'question': '\\n[STATEMENT]\\nAs initial conditions I have that, the red block is clear, the orange block is clear, the hand is empty, the orange block is on top of the blue block, the red block is on the table and the blue block is on the table.\\nMy goal is to have that the red block is on top of the orange block.\\n\\nMy plan is as follows:\\n\\n[PLAN]\\n', 'instance_file': 'LLMs-Planning/llm_planning_analysis/instances/blocksworld/generated_basic_3/instance-45.pddl'};accuracy=0.892 (33/37)\n",
      "blocksworld:  79%|██████████████████████      | 37/47 [1:04:56<15:43, 94.34s/it]\n",
      "MCTS iteration:   0%|                                    | 0/10 [00:00<?, ?it/s]\u001b[A/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Error: zero-colors\n",
      "\n",
      "/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "\n",
      "MCTS iteration:  10%|██▊                         | 1/10 [01:30<13:30, 90.11s/it]\u001b[A\n",
      "                                                                                \u001b[A[+]: Saving plan in tmp_plan.txt\n",
      "RESPONSE::: Checking plan: tmp_plan.txt\n",
      "Plan executed successfully - checking goal\n",
      "Plan valid\n",
      "Final value: 2 \n",
      "\n",
      "Successful plans:\n",
      "Value: 2\n",
      " tmp_plan.txt 2 \n",
      "\n",
      "\n",
      "Case #38: correct=True, output='pick up the orange block\\nstack the orange block on top of the blue block', answer={'init': 'the blue block is clear, the orange block is clear, the hand is empty, the blue block is on top of the red block, the red block is on the table and the orange block is on the table', 'goal': 'the orange block is on top of the blue block', 'plan': '\\npick up the orange block\\nstack the orange block on top of the blue block\\n[PLAN END]\\n', 'question': '\\n[STATEMENT]\\nAs initial conditions I have that, the blue block is clear, the orange block is clear, the hand is empty, the blue block is on top of the red block, the red block is on the table and the orange block is on the table.\\nMy goal is to have that the orange block is on top of the blue block.\\n\\nMy plan is as follows:\\n\\n[PLAN]\\n', 'instance_file': 'LLMs-Planning/llm_planning_analysis/instances/blocksworld/generated_basic_3/instance-86.pddl'};accuracy=0.895 (34/38)\n",
      "blocksworld:  81%|██████████████████████▋     | 38/47 [1:06:27<14:01, 93.52s/it]\n",
      "MCTS iteration:   0%|                                    | 0/10 [00:00<?, ?it/s]\u001b[A/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "\n",
      "MCTS iteration:  10%|██▊                         | 1/10 [01:30<13:33, 90.37s/it]\u001b[A\n",
      "                                                                                \u001b[A[+]: Saving plan in tmp_plan.txt\n",
      "RESPONSE::: Checking plan: tmp_plan.txt\n",
      "Plan executed successfully - checking goal\n",
      "Plan valid\n",
      "Final value: 2 \n",
      "\n",
      "Successful plans:\n",
      "Value: 2\n",
      " tmp_plan.txt 2 \n",
      "\n",
      "\n",
      "Case #39: correct=True, output='pick up the orange block\\nstack the orange block on top of the blue block', answer={'init': 'the blue block is clear, the orange block is clear, the hand is empty, the blue block is on top of the red block, the red block is on the table and the orange block is on the table', 'goal': 'the blue block is on top of the red block and the orange block is on top of the blue block', 'plan': '\\npick up the orange block\\nstack the orange block on top of the blue block\\n[PLAN END]\\n', 'question': '\\n[STATEMENT]\\nAs initial conditions I have that, the blue block is clear, the orange block is clear, the hand is empty, the blue block is on top of the red block, the red block is on the table and the orange block is on the table.\\nMy goal is to have that the blue block is on top of the red block and the orange block is on top of the blue block.\\n\\nMy plan is as follows:\\n\\n[PLAN]\\n', 'instance_file': 'LLMs-Planning/llm_planning_analysis/instances/blocksworld/generated_basic_3/instance-41.pddl'};accuracy=0.897 (35/39)\n",
      "blocksworld:  83%|███████████████████████▏    | 39/47 [1:07:59<12:24, 93.06s/it]\n",
      "MCTS iteration:   0%|                                    | 0/10 [00:00<?, ?it/s]\u001b[A/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Error: zero-colors\n",
      "\n",
      "/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "\n",
      "MCTS iteration:  10%|██▊                         | 1/10 [01:35<14:22, 95.88s/it]\u001b[A/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "\n",
      "MCTS iteration:  30%|████████▍                   | 3/10 [03:11<07:01, 60.15s/it]\u001b[A\n",
      "                                                                                \u001b[A[+]: Saving plan in tmp_plan.txt\n",
      "RESPONSE::: Checking plan: tmp_plan.txt\n",
      "Plan executed successfully - checking goal\n",
      "Plan valid\n",
      "Final value: 2 \n",
      "\n",
      "Successful plans:\n",
      "Value: 2\n",
      " tmp_plan.txt 2 \n",
      "\n",
      "\n",
      "Case #40: correct=True, output='unstack the blue block from on top of the red block\\nstack the blue block on top of the orange block', answer={'init': 'the blue block is clear, the orange block is clear, the hand is empty, the blue block is on top of the red block, the red block is on the table and the orange block is on the table', 'goal': 'the blue block is on top of the orange block', 'plan': '\\nunstack the blue block from on top of the red block\\nstack the blue block on top of the orange block\\n[PLAN END]\\n', 'question': '\\n[STATEMENT]\\nAs initial conditions I have that, the blue block is clear, the orange block is clear, the hand is empty, the blue block is on top of the red block, the red block is on the table and the orange block is on the table.\\nMy goal is to have that the blue block is on top of the orange block.\\n\\nMy plan is as follows:\\n\\n[PLAN]\\n', 'instance_file': 'LLMs-Planning/llm_planning_analysis/instances/blocksworld/generated_basic_3/instance-96.pddl'};accuracy=0.900 (36/40)\n",
      "blocksworld:  85%|██████████████████████▉    | 40/47 [1:11:12<14:21, 123.01s/it]\n",
      "MCTS iteration:   0%|                                    | 0/10 [00:00<?, ?it/s]\u001b[A/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "\n",
      "MCTS iteration:  10%|██▊                         | 1/10 [01:35<14:22, 95.81s/it]\u001b[A/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "\n",
      "MCTS iteration:  20%|█████▌                      | 2/10 [03:10<12:40, 95.08s/it]\u001b[A\n",
      "                                                                                \u001b[A[+]: Saving plan in tmp_plan.txt\n",
      "RESPONSE::: Checking plan: tmp_plan.txt\n",
      "Plan executed successfully - checking goal\n",
      "Plan valid\n",
      "Final value: 2 \n",
      "\n",
      "Successful plans:\n",
      "Value: 2\n",
      " tmp_plan.txt 2 \n",
      "\n",
      "\n",
      "Case #41: correct=True, output='unstack the blue block from on top of the orange block\\nstack the blue block on top of the red block', answer={'init': 'the red block is clear, the blue block is clear, the hand is empty, the blue block is on top of the orange block, the red block is on the table and the orange block is on the table', 'goal': 'the blue block is on top of the red block', 'plan': '\\nunstack the blue block from on top of the orange block\\nstack the blue block on top of the red block\\n[PLAN END]\\n', 'question': '\\n[STATEMENT]\\nAs initial conditions I have that, the red block is clear, the blue block is clear, the hand is empty, the blue block is on top of the orange block, the red block is on the table and the orange block is on the table.\\nMy goal is to have that the blue block is on top of the red block.\\n\\nMy plan is as follows:\\n\\n[PLAN]\\n', 'instance_file': 'LLMs-Planning/llm_planning_analysis/instances/blocksworld/generated_basic_3/instance-92.pddl'};accuracy=0.902 (37/41)\n",
      "blocksworld:  87%|███████████████████████▌   | 41/47 [1:14:24<14:22, 143.73s/it]\n",
      "MCTS iteration:   0%|                                    | 0/10 [00:00<?, ?it/s]\u001b[A/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Error: zero-colors\n",
      "i have that\n",
      "/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "\n",
      "MCTS iteration:  10%|██▊                         | 1/10 [01:29<13:29, 89.93s/it]\u001b[A\n",
      "                                                                                \u001b[A[+]: Saving plan in tmp_plan.txt\n",
      "RESPONSE::: Checking plan: tmp_plan.txt\n",
      "Plan executed successfully - checking goal\n",
      "Plan valid\n",
      "Final value: 2 \n",
      "\n",
      "Successful plans:\n",
      "Value: 2\n",
      " tmp_plan.txt 2 \n",
      "\n",
      "\n",
      "Case #42: correct=True, output='pick up the red block\\nstack the red block on top of the blue block', answer={'init': 'the red block is clear, the blue block is clear, the hand is empty, the blue block is on top of the orange block, the red block is on the table and the orange block is on the table', 'goal': 'the red block is on top of the blue block and the blue block is on top of the orange block', 'plan': '\\npick up the red block\\nstack the red block on top of the blue block\\n[PLAN END]\\n', 'question': '\\n[STATEMENT]\\nAs initial conditions I have that, the red block is clear, the blue block is clear, the hand is empty, the blue block is on top of the orange block, the red block is on the table and the orange block is on the table.\\nMy goal is to have that the red block is on top of the blue block and the blue block is on top of the orange block.\\n\\nMy plan is as follows:\\n\\n[PLAN]\\n', 'instance_file': 'LLMs-Planning/llm_planning_analysis/instances/blocksworld/generated_basic_3/instance-48.pddl'};accuracy=0.905 (38/42)\n",
      "blocksworld:  89%|████████████████████████▏  | 42/47 [1:15:56<10:40, 128.15s/it]\n",
      "MCTS iteration:   0%|                                    | 0/10 [00:00<?, ?it/s]\u001b[A/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Error: zero-colors\n",
      "\n",
      "\n",
      "MCTS iteration:  10%|██▊                         | 1/10 [01:32<13:49, 92.17s/it]\u001b[A\n",
      "                                                                                \u001b[A[+]: Saving plan in tmp_plan.txt\n",
      "RESPONSE::: Checking plan: tmp_plan.txt\n",
      "Plan executed successfully - checking goal\n",
      "Plan valid\n",
      "Final value: 2 \n",
      "\n",
      "Successful plans:\n",
      "Value: 2\n",
      " tmp_plan.txt 2 \n",
      "\n",
      "\n",
      "Case #43: correct=True, output='pick up the orange block\\nstack the orange block on top of the red block', answer={'init': 'the red block is clear, the orange block is clear, the hand is empty, the red block is on top of the blue block, the blue block is on the table and the orange block is on the table', 'goal': 'the orange block is on top of the red block', 'plan': '\\npick up the orange block\\nstack the orange block on top of the red block\\n[PLAN END]\\n', 'question': '\\n[STATEMENT]\\nAs initial conditions I have that, the red block is clear, the orange block is clear, the hand is empty, the red block is on top of the blue block, the blue block is on the table and the orange block is on the table.\\nMy goal is to have that the orange block is on top of the red block.\\n\\nMy plan is as follows:\\n\\n[PLAN]\\n', 'instance_file': 'LLMs-Planning/llm_planning_analysis/instances/blocksworld/generated_basic_3/instance-33.pddl'};accuracy=0.907 (39/43)\n",
      "blocksworld:  91%|████████████████████████▋  | 43/47 [1:17:30<07:51, 117.83s/it]\n",
      "MCTS iteration:   0%|                                    | 0/10 [00:00<?, ?it/s]\u001b[A/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Error: zero-colors\n",
      "i have that\n",
      "/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "\n",
      "MCTS iteration:  10%|██▊                         | 1/10 [01:31<13:45, 91.68s/it]\u001b[A\n",
      "                                                                                \u001b[A[+]: Saving plan in tmp_plan.txt\n",
      "RESPONSE::: Checking plan: tmp_plan.txt\n",
      "Plan executed successfully - checking goal\n",
      "Goal not satisfied\n",
      "Plan invalid\n",
      "\n",
      "\n",
      "Failed plans:\n",
      " tmp_plan.txt \n",
      "\n",
      "Case #44: correct=False, output='pick up the blue block\\nstack the blue block on top of the red block', answer={'init': 'the red block is clear, the blue block is clear, the hand is empty, the red block is on top of the orange block, the blue block is on the table and the orange block is on the table', 'goal': 'the red block is on top of the blue block', 'plan': '\\nunstack the red block from on top of the orange block\\nstack the red block on top of the blue block\\n[PLAN END]\\n', 'question': '\\n[STATEMENT]\\nAs initial conditions I have that, the red block is clear, the blue block is clear, the hand is empty, the red block is on top of the orange block, the blue block is on the table and the orange block is on the table.\\nMy goal is to have that the red block is on top of the blue block.\\n\\nMy plan is as follows:\\n\\n[PLAN]\\n', 'instance_file': 'LLMs-Planning/llm_planning_analysis/instances/blocksworld/generated_basic_3/instance-47.pddl'};accuracy=0.886 (39/44)\n",
      "blocksworld:  94%|█████████████████████████▎ | 44/47 [1:19:03<05:31, 110.47s/it]\n",
      "MCTS iteration:   0%|                                    | 0/10 [00:00<?, ?it/s]\u001b[A/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "\n",
      "MCTS iteration:  10%|██▊                         | 1/10 [01:32<13:51, 92.34s/it]\u001b[A\n",
      "                                                                                \u001b[A[+]: Saving plan in tmp_plan.txt\n",
      "RESPONSE::: Checking plan: tmp_plan.txt\n",
      "Plan executed successfully - checking goal\n",
      "Plan valid\n",
      "Final value: 2 \n",
      "\n",
      "Successful plans:\n",
      "Value: 2\n",
      " tmp_plan.txt 2 \n",
      "\n",
      "\n",
      "Case #45: correct=True, output='pick up the blue block\\nstack the blue block on top of the orange block', answer={'init': 'the red block is clear, the blue block is clear, the orange block is clear, the hand is empty, the red block is on the table, the blue block is on the table and the orange block is on the table', 'goal': 'the blue block is on top of the orange block', 'plan': '\\npick up the blue block\\nstack the blue block on top of the orange block\\n[PLAN END]\\n', 'question': '\\n[STATEMENT]\\nAs initial conditions I have that, the red block is clear, the blue block is clear, the orange block is clear, the hand is empty, the red block is on the table, the blue block is on the table and the orange block is on the table.\\nMy goal is to have that the blue block is on top of the orange block.\\n\\nMy plan is as follows:\\n\\n[PLAN]\\n', 'instance_file': 'LLMs-Planning/llm_planning_analysis/instances/blocksworld/generated_basic_3/instance-77.pddl'};accuracy=0.889 (40/45)\n",
      "blocksworld:  96%|█████████████████████████▊ | 45/47 [1:20:37<03:30, 105.50s/it]\n",
      "MCTS iteration:   0%|                                    | 0/10 [00:00<?, ?it/s]\u001b[A/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Error: zero-colors\n",
      "\n",
      "/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "\n",
      "MCTS iteration:  10%|██▊                         | 1/10 [01:34<14:09, 94.37s/it]\u001b[A\n",
      "                                                                                \u001b[A[+]: Saving plan in tmp_plan.txt\n",
      "RESPONSE::: Checking plan: tmp_plan.txt\n",
      "Plan executed successfully - checking goal\n",
      "Plan valid\n",
      "Final value: 2 \n",
      "\n",
      "Successful plans:\n",
      "Value: 2\n",
      " tmp_plan.txt 2 \n",
      "\n",
      "\n",
      "Case #46: correct=True, output='pick up the blue block\\nstack the blue block on top of the red block', answer={'init': 'the red block is clear, the blue block is clear, the orange block is clear, the hand is empty, the red block is on the table, the blue block is on the table and the orange block is on the table', 'goal': 'the blue block is on top of the red block', 'plan': '\\npick up the blue block\\nstack the blue block on top of the red block\\n[PLAN END]\\n', 'question': '\\n[STATEMENT]\\nAs initial conditions I have that, the red block is clear, the blue block is clear, the orange block is clear, the hand is empty, the red block is on the table, the blue block is on the table and the orange block is on the table.\\nMy goal is to have that the blue block is on top of the red block.\\n\\nMy plan is as follows:\\n\\n[PLAN]\\n', 'instance_file': 'LLMs-Planning/llm_planning_analysis/instances/blocksworld/generated_basic_3/instance-89.pddl'};accuracy=0.891 (41/46)\n",
      "blocksworld:  98%|██████████████████████████▍| 46/47 [1:22:13<01:42, 102.65s/it]\n",
      "MCTS iteration:   0%|                                    | 0/10 [00:00<?, ?it/s]\u001b[A/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/workspace/assets/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
      "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "\n",
      "MCTS iteration:  10%|██▊                         | 1/10 [01:32<13:55, 92.79s/it]\u001b[A\n",
      "                                                                                \u001b[A[+]: Saving plan in tmp_plan.txt\n",
      "RESPONSE::: Checking plan: tmp_plan.txt\n",
      "Plan executed successfully - checking goal\n",
      "Plan valid\n",
      "Final value: 2 \n",
      "\n",
      "Successful plans:\n",
      "Value: 2\n",
      " tmp_plan.txt 2 \n",
      "\n",
      "\n",
      "Case #47: correct=True, output='pick up the blue block\\nstack the blue block on top of the red block', answer={'init': 'the red block is clear, the blue block is clear, the hand is empty, the red block is on top of the orange block, the blue block is on the table and the orange block is on the table', 'goal': 'the red block is on top of the orange block and the blue block is on top of the red block', 'plan': '\\npick up the blue block\\nstack the blue block on top of the red block\\n[PLAN END]\\n', 'question': '\\n[STATEMENT]\\nAs initial conditions I have that, the red block is clear, the blue block is clear, the hand is empty, the red block is on top of the orange block, the blue block is on the table and the orange block is on the table.\\nMy goal is to have that the red block is on top of the orange block and the blue block is on top of the red block.\\n\\nMy plan is as follows:\\n\\n[PLAN]\\n', 'instance_file': 'LLMs-Planning/llm_planning_analysis/instances/blocksworld/generated_basic_3/instance-83.pddl'};accuracy=0.894 (42/47)\n",
      "blocksworld: 100%|███████████████████████████| 47/47 [1:23:47<00:00, 106.97s/it]\n",
      "0.8936170212765957\n"
     ]
    }
   ],
   "source": [
    "# /content/llm-reasoners_tutorial/reasoners/lm/hf_model.py の98行目をコメントアウトする\n",
    "# assets/llm-reasoners_tutorial/examples/rap_blocksworld/inference.pyの235行目をfire.Fire(llama_hf_main)に変更\n",
    "# prompt_pathをpool_prompt_v2_step_*.jsonにする\n",
    "# rap_inference.pyのrap_を消す\n",
    "\n",
    "!python examples/blocksworld/rap_inference.py --data_path 'examples/blocksworld/data/full_data/step_2.json' --prompt_path 'examples/blocksworld/prompts/pool_prompt_v2_step_2.json' --depth_limit 2 --llama_path \"meta-llama/Llama-2-7b-hf\" --peft_path None --batch_size 1 --quantized 'nf4' --output_trace_in_each_iter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python examples/blocksworld/rap_inference.py --data_path 'examples/blocksworld/data/full_data/step_4.json' --prompt_path 'examples/blocksworld/prompts/pool_prompt_v2_step_4.json' --depth_limit 4 --llama_path \"meta-llama/Llama-2-7b-hf\" --peft_path None --batch_size 1 --quantized 'nf4' --output_trace_in_each_iter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/assets/llm-reasoners_main/examples/blocksworld/rap_inference.py\", line 9, in <module>\n",
      "    from reasoners.benchmark import BWEvaluator\n",
      "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1002, in _find_and_load_unlocked\n"
     ]
    }
   ],
   "source": [
    "!python examples/blocksworld/rap_inference.py --data_path 'examples/blocksworld/data/full_data/step_6.json' --prompt_path 'examples/blocksworld/prompts/pool_prompt_v2_step_6.json' --depth_limit 6 --llama_path \"meta-llama/Llama-2-7b-hf\" --peft_path None --batch_size 8 --quantized 'nf4' --output_trace_in_each_iter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
