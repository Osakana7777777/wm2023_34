{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UiYHuHEyGm0u",
        "outputId": "c4c70d94-49db-4227-bc51-37a084d4c810"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tue Jan  9 00:34:03 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   48C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2wOfzuD1dyn",
        "outputId": "454c1961-b5e1-457d-f092-f46911e4323d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'llm-reasoners_main'...\n",
            "remote: Enumerating objects: 4085, done.\u001b[K\n",
            "remote: Counting objects: 100% (2766/2766), done.\u001b[K\n",
            "remote: Compressing objects: 100% (960/960), done.\u001b[K\n",
            "remote: Total 4085 (delta 1912), reused 2561 (delta 1789), pack-reused 1319\u001b[K\n",
            "Receiving objects: 100% (4085/4085), 16.31 MiB | 16.54 MiB/s, done.\n",
            "Resolving deltas: 100% (2731/2731), done.\n",
            "Submodule 'LLMs-Planning' (https://github.com/karthikv792/LLMs-Planning) registered for path 'LLMs-Planning'\n",
            "Submodule 'exllama' (https://github.com/turboderp/exllama.git) registered for path 'exllama'\n",
            "Cloning into '/content/llm-reasoners_main/LLMs-Planning'...\n",
            "remote: Enumerating objects: 7273, done.        \n",
            "remote: Counting objects: 100% (1226/1226), done.        \n",
            "remote: Compressing objects: 100% (259/259), done.        \n",
            "remote: Total 7273 (delta 1077), reused 1060 (delta 965), pack-reused 6047        \n",
            "Receiving objects: 100% (7273/7273), 31.20 MiB | 15.06 MiB/s, done.\n",
            "Resolving deltas: 100% (4449/4449), done.\n",
            "Cloning into '/content/llm-reasoners_main/exllama'...\n",
            "remote: Enumerating objects: 1523, done.        \n",
            "remote: Counting objects: 100% (853/853), done.        \n",
            "remote: Compressing objects: 100% (299/299), done.        \n",
            "remote: Total 1523 (delta 679), reused 589 (delta 553), pack-reused 670        \n",
            "Receiving objects: 100% (1523/1523), 950.59 KiB | 14.62 MiB/s, done.\n",
            "Resolving deltas: 100% (1061/1061), done.\n",
            "Submodule path 'LLMs-Planning': checked out '34e6841f81ca7708f2f8b8241504bfe8a908e40b'\n",
            "Submodule path 'exllama': checked out '74e0ddd69fde8b7887552e6c23b88c418cf7e713'\n"
          ]
        }
      ],
      "source": [
        "!git clone -b main https://github.com/Ber666/llm-reasoners.git --recursive llm-reasoners_main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sj-ANovHTVhD",
        "outputId": "9a328b70-839a-4f03-b3c0-328da93d7c66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/llm-reasoners_main\n"
          ]
        }
      ],
      "source": [
        "%cd llm-reasoners_main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gl0JAZvGTjB2",
        "outputId": "679ed41f-15db-4b80-ad97-8c0f77e7c7ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Obtaining file:///content/llm-reasoners_main\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting llama1@ git+https://github.com/AegeanYan/llama@llama_v1 (from reasoners==0.0.0)\n",
            "  Cloning https://github.com/AegeanYan/llama (to revision llama_v1) to /tmp/pip-install-t3fi6zlp/llama1_6d916e8b336f4f78a33b7fc77c1d1da0\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/AegeanYan/llama /tmp/pip-install-t3fi6zlp/llama1_6d916e8b336f4f78a33b7fc77c1d1da0\n",
            "  Running command git checkout -b llama_v1 --track origin/llama_v1\n",
            "  Switched to a new branch 'llama_v1'\n",
            "  Branch 'llama_v1' set up to track remote branch 'llama_v1' from 'origin'.\n",
            "  Resolved https://github.com/AegeanYan/llama to commit bbb5c8e861e25e073bc4d3356152b1ac8422aa09\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting llama@ git+https://github.com/facebookresearch/llama@main (from reasoners==0.0.0)\n",
            "  Cloning https://github.com/facebookresearch/llama (to revision main) to /tmp/pip-install-t3fi6zlp/llama_06f0f8cd6a36476c9cc9c1dbfbb2c57d\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/llama /tmp/pip-install-t3fi6zlp/llama_06f0f8cd6a36476c9cc9c1dbfbb2c57d\n",
            "  Resolved https://github.com/facebookresearch/llama to commit ef351e9cd9496c579bf9f2bb036ef11bdc5ca3d2\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from reasoners==0.0.0) (4.66.1)\n",
            "Collecting fire (from reasoners==0.0.0)\n",
            "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from reasoners==0.0.0) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from reasoners==0.0.0) (1.11.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from reasoners==0.0.0) (2.1.0+cu121)\n",
            "Collecting datasets (from reasoners==0.0.0)\n",
            "  Downloading datasets-2.16.1-py3-none-any.whl (507 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from reasoners==0.0.0) (0.20.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from reasoners==0.0.0) (4.35.2)\n",
            "Collecting sentencepiece (from reasoners==0.0.0)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai (from reasoners==0.0.0)\n",
            "  Downloading openai-1.6.1-py3-none-any.whl (225 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.4/225.4 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tarski (from reasoners==0.0.0)\n",
            "  Downloading tarski-0.8.2-py3-none-any.whl (213 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.1/213.1 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting peft (from reasoners==0.0.0)\n",
            "  Downloading peft-0.7.1-py3-none-any.whl (168 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.3/168.3 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting optimum (from reasoners==0.0.0)\n",
            "  Downloading optimum-1.16.1-py3-none-any.whl (403 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m403.3/403.3 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ninja (from reasoners==0.0.0)\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bitsandbytes (from reasoners==0.0.0)\n",
            "  Downloading bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fairscale (from reasoners==0.0.0)\n",
            "  Downloading fairscale-0.4.13.tar.gz (266 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.3/266.3 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets->reasoners==0.0.0) (3.13.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->reasoners==0.0.0) (10.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->reasoners==0.0.0) (0.6)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets->reasoners==0.0.0)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->reasoners==0.0.0) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets->reasoners==0.0.0) (2.31.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->reasoners==0.0.0) (3.4.1)\n",
            "Collecting multiprocess (from datasets->reasoners==0.0.0)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets->reasoners==0.0.0) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->reasoners==0.0.0) (3.9.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets->reasoners==0.0.0) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets->reasoners==0.0.0) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->reasoners==0.0.0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->reasoners==0.0.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->reasoners==0.0.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->reasoners==0.0.0) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->reasoners==0.0.0) (2.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire->reasoners==0.0.0) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->reasoners==0.0.0) (2.4.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai->reasoners==0.0.0) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai->reasoners==0.0.0) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai->reasoners==0.0.0)\n",
            "  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai->reasoners==0.0.0) (1.10.13)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai->reasoners==0.0.0) (1.3.0)\n",
            "Collecting typing-extensions>=3.7.4.3 (from huggingface_hub->reasoners==0.0.0)\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Collecting coloredlogs (from optimum->reasoners==0.0.0)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft->reasoners==0.0.0) (5.9.5)\n",
            "Collecting accelerate>=0.21.0 (from peft->reasoners==0.0.0)\n",
            "  Downloading accelerate-0.25.0-py3-none-any.whl (265 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft->reasoners==0.0.0) (0.4.1)\n",
            "Requirement already satisfied: multipledispatch in /usr/local/lib/python3.10/dist-packages (from tarski->reasoners==0.0.0) (1.0.0)\n",
            "Collecting antlr4-python3-runtime==4.7.2 (from tarski->reasoners==0.0.0)\n",
            "  Downloading antlr4-python3-runtime-4.7.2.tar.gz (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.3/112.3 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->reasoners==0.0.0) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers->reasoners==0.0.0) (0.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai->reasoners==0.0.0) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai->reasoners==0.0.0) (1.2.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->reasoners==0.0.0) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->reasoners==0.0.0) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->reasoners==0.0.0) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->reasoners==0.0.0) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->reasoners==0.0.0) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->reasoners==0.0.0) (4.0.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai->reasoners==0.0.0) (2023.11.17)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai->reasoners==0.0.0)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai->reasoners==0.0.0)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->reasoners==0.0.0) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->reasoners==0.0.0) (2.0.7)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers->reasoners==0.0.0) (3.20.3)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->optimum->reasoners==0.0.0)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->reasoners==0.0.0) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->reasoners==0.0.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->reasoners==0.0.0) (2023.3.post1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->reasoners==0.0.0) (1.3.0)\n",
            "Building wheels for collected packages: fairscale, fire, llama, llama1, antlr4-python3-runtime\n",
            "  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairscale: filename=fairscale-0.4.13-py3-none-any.whl size=332104 sha256=68ccf4c4c29f1f2841786b7a434448dbc3c5f1e92aea184fabd5a2ca51e1f418\n",
            "  Stored in directory: /root/.cache/pip/wheels/78/a4/c0/fb0a7ef03cff161611c3fa40c6cf898f76e58ec421b88e8cb3\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116934 sha256=f4641b9952faf66aea4c7308e030ed25be32c0527c992aaf343f2a29b90087f6\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/d4/f7/9404e5db0116bd4d43e5666eaa3e70ab53723e1e3ea40c9a95\n",
            "  Building wheel for llama (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama: filename=llama-0.0.1-py3-none-any.whl size=14380 sha256=9795e3b5235a34b59fb7f0233f7bfacd4c1c38366ca70f427edc370bbce74bdf\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-xrnm3_kn/wheels/45/12/a6/4b0afb0097259ac4aa71be3eaf224bead3a32c3ac64fa61af7\n",
            "  Building wheel for llama1 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama1: filename=llama1-0.0.0-py3-none-any.whl size=17887 sha256=594251c00ca7d91f66449a9750421ba71f68caa08f07947ba07bd2c16ed1ac57\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-xrnm3_kn/wheels/c8/44/d8/8e82fa63fbc38bbbbb9be3d6e2c514b67914e0ff635af71aee\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.7.2-py3-none-any.whl size=140930 sha256=ba48e8bd7759d2412789e24c0cf2135dac7d6ae1815bff700e4d3d8578b8eb06\n",
            "  Stored in directory: /root/.cache/pip/wheels/79/20/ec/30bf7dabc29319ccc0d0c96f910a640513a3c81faa960fed43\n",
            "Successfully built fairscale fire llama llama1 antlr4-python3-runtime\n",
            "Installing collected packages: sentencepiece, ninja, llama1, antlr4-python3-runtime, typing-extensions, tarski, humanfriendly, h11, fire, dill, multiprocess, httpcore, coloredlogs, bitsandbytes, httpx, fairscale, accelerate, openai, llama, datasets, peft, optimum, reasoners\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "  Running setup.py develop for reasoners\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-0.25.0 antlr4-python3-runtime-4.7.2 bitsandbytes-0.42.0 coloredlogs-15.0.1 datasets-2.16.1 dill-0.3.7 fairscale-0.4.13 fire-0.5.0 h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 humanfriendly-10.0 llama-0.0.1 llama1-0.0.0 multiprocess-0.70.15 ninja-1.11.1.1 openai-1.6.1 optimum-1.16.1 peft-0.7.1 reasoners sentencepiece-0.1.99 tarski-0.8.2 typing-extensions-4.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "s1Qoqb9tCJro",
        "outputId": "f5f438c7-3ef2-4c71-dc09-d312d64666c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/facebookresearch/llama@llama_v1 (from -r requirements.txt (line 11))\n",
            "  Cloning https://github.com/facebookresearch/llama (to revision llama_v1) to /tmp/pip-req-build-2sparhde\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/llama /tmp/pip-req-build-2sparhde\n",
            "  Running command git checkout -b llama_v1 --track origin/llama_v1\n",
            "  Switched to a new branch 'llama_v1'\n",
            "  Branch 'llama_v1' set up to track remote branch 'llama_v1' from 'origin'.\n",
            "  Resolved https://github.com/facebookresearch/llama to commit 57b0eb62de0636e75af471e49e2f1862d908d9d8\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (4.66.1)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (0.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (1.11.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (2.1.0+cu121)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (2.16.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (4.35.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (0.1.99)\n",
            "Requirement already satisfied: fairscale in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (0.4.13)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (1.6.1)\n",
            "Requirement already satisfied: requests~=2.31.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 16)) (2.31.0)\n",
            "Requirement already satisfied: PyYAML~=6.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 17)) (6.0.1)\n",
            "Requirement already satisfied: tarski~=0.8.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 18)) (0.8.2)\n",
            "Collecting setuptools~=67.8.0 (from -r requirements.txt (line 19))\n",
            "  Downloading setuptools-67.8.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting peft~=0.5.0 (from -r requirements.txt (line 23))\n",
            "  Downloading peft-0.5.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.6/85.6 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: optimum in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 24)) (1.16.1)\n",
            "Requirement already satisfied: ninja~=1.11.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 25)) (1.11.1.1)\n",
            "Collecting bitsandbytes~=0.41.1 (from -r requirements.txt (line 26))\n",
            "  Downloading bitsandbytes-0.41.3.post2-py3-none-any.whl (92.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire->-r requirements.txt (line 2)) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->-r requirements.txt (line 2)) (2.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 5)) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 5)) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 5)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 5)) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 5)) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 5)) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 5)) (2.1.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 6)) (10.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 6)) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 6)) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 6)) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 6)) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 6)) (0.70.15)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 6)) (3.9.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 6)) (0.20.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 6)) (23.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 8)) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 8)) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 8)) (0.4.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai->-r requirements.txt (line 14)) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai->-r requirements.txt (line 14)) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai->-r requirements.txt (line 14)) (0.26.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai->-r requirements.txt (line 14)) (1.10.13)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai->-r requirements.txt (line 14)) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests~=2.31.0->-r requirements.txt (line 16)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests~=2.31.0->-r requirements.txt (line 16)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests~=2.31.0->-r requirements.txt (line 16)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests~=2.31.0->-r requirements.txt (line 16)) (2023.11.17)\n",
            "Requirement already satisfied: multipledispatch in /usr/local/lib/python3.10/dist-packages (from tarski~=0.8.2->-r requirements.txt (line 18)) (1.0.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.7.2 in /usr/local/lib/python3.10/dist-packages (from tarski~=0.8.2->-r requirements.txt (line 18)) (4.7.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from tarski~=0.8.2->-r requirements.txt (line 18)) (5.9.5)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from peft~=0.5.0->-r requirements.txt (line 23)) (0.25.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from optimum->-r requirements.txt (line 24)) (15.0.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai->-r requirements.txt (line 14)) (1.2.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 6)) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 6)) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 6)) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 6)) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 6)) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 6)) (4.0.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 14)) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r requirements.txt (line 14)) (0.14.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 8)) (3.20.3)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->optimum->-r requirements.txt (line 24)) (10.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r requirements.txt (line 5)) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r requirements.txt (line 6)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r requirements.txt (line 6)) (2023.3.post1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->-r requirements.txt (line 5)) (1.3.0)\n",
            "Building wheels for collected packages: llama\n",
            "  Building wheel for llama (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama: filename=llama-0.0.0-py3-none-any.whl size=17862 sha256=d5efccb379f13bf7a4796182a23f4b226f8c5a7ab26cd2d4c9fb84e5f3572459\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-izh6uv04/wheels/2b/52/58/73f1ca5bfa254c328b923c8955c028e4403180909cb232e560\n",
            "Successfully built llama\n",
            "Installing collected packages: llama, bitsandbytes, setuptools, peft\n",
            "  Attempting uninstall: llama\n",
            "    Found existing installation: llama 0.0.1\n",
            "    Uninstalling llama-0.0.1:\n",
            "      Successfully uninstalled llama-0.0.1\n",
            "  Attempting uninstall: bitsandbytes\n",
            "    Found existing installation: bitsandbytes 0.42.0\n",
            "    Uninstalling bitsandbytes-0.42.0:\n",
            "      Successfully uninstalled bitsandbytes-0.42.0\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 67.7.2\n",
            "    Uninstalling setuptools-67.7.2:\n",
            "      Successfully uninstalled setuptools-67.7.2\n",
            "  Attempting uninstall: peft\n",
            "    Found existing installation: peft 0.7.1\n",
            "    Uninstalling peft-0.7.1:\n",
            "      Successfully uninstalled peft-0.7.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed bitsandbytes-0.41.3.post2 llama-0.0.0 peft-0.5.0 setuptools-67.8.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack",
                  "pkg_resources",
                  "setuptools"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AaOu1Or7EZZf",
        "outputId": "b2bd5a4d-33c9-4980-e5eb-7cfad74cd55f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "!pip install -q huggingface_hub\n",
        "import huggingface_hub\n",
        "huggingface_hub.login(token=\"your token\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfF1fWLz2H9c",
        "outputId": "f96a963c-7d0d-4163-9da3-91bef57b4547"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maTrB6nc2MVF",
        "outputId": "d7a5d69a-4d7a-454c-9f44-7ade22529156"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'LLMs-Planning'...\n",
            "remote: Enumerating objects: 7273, done.\u001b[K\n",
            "remote: Counting objects: 100% (1226/1226), done.\u001b[K\n",
            "remote: Compressing objects: 100% (259/259), done.\u001b[K\n",
            "remote: Total 7273 (delta 1077), reused 1060 (delta 965), pack-reused 6047\u001b[K\n",
            "Receiving objects: 100% (7273/7273), 31.20 MiB | 15.79 MiB/s, done.\n",
            "Resolving deltas: 100% (4449/4449), done.\n",
            "Updating files: 100% (8893/8893), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/karthikv792/LLMs-Planning.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b08xXye3FFm",
        "outputId": "a099e220-c11e-4847-a008-67941d6defef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/llm-reasoners_main\n"
          ]
        }
      ],
      "source": [
        "%cd /content/llm-reasoners_main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OFzG6GmefJ4",
        "outputId": "2fa63496-dc3c-44af-fbf7-5a14d892117b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/50.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/111.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.6/111.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q pddl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JuxmoAf9qS0i"
      },
      "outputs": [],
      "source": [
        "# prompt: 環境変数VALを設定する\n",
        "import os\n",
        "os.environ['VAL'] = '/content/LLMs-Planning/planner_tools/VAL'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aW_7c6aDrpPp",
        "outputId": "676329b8-9a7a-430d-b937-f53d8f3b2dc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/LLMs-Planning/planner_tools/VAL\n"
          ]
        }
      ],
      "source": [
        "# prompt: 環境変数VALの値を見る\n",
        "\n",
        "print(os.environ.get('VAL'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "FarIKkN6U6ka",
        "outputId": "b41394d6-c091-4692-e238-27b1bc6043f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-01-09 00:52:08.705137: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-09 00:52:08.705198: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-09 00:52:08.706506: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-09 00:52:10.058709: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "quantizing.............................\n",
            "Loading checkpoint shards: 100% 2/2 [00:58<00:00, 29.34s/it]\n",
            "blocksworld:   0% 0/47 [00:00<?, ?it/s]\n",
            "MCTS iteration:   0% 0/10 [00:00<?, ?it/s]\u001b[A/content/llm-reasoners_main/reasoners/lm/hf_model.py:137: UserWarning: the eos_token '\\n' is encoded into [29871, 13] with length != 1, using 13 as the eos_token_id\n",
            "  warnings.warn(f'the eos_token {repr(token)} is encoded into {tokenized} with length != 1, '\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "\n",
            "MCTS iteration:  10% 1/10 [02:45<24:52, 165.84s/it]\u001b[A\n",
            "MCTS iteration:  20% 2/10 [03:38<13:13, 99.21s/it] \u001b[A\n",
            "                                                  \u001b[A[+]: Saving plan in tmp_plan.txt\n",
            "RESPONSE::: Checking plan: tmp_plan.txt\n",
            "Plan executed successfully - checking goal\n",
            "Plan valid\n",
            "Final value: 2 \n",
            "\n",
            "Successful plans:\n",
            "Value: 2\n",
            " tmp_plan.txt 2 \n",
            "\n",
            "\n",
            "Case #1: correct=True, output='unstack the yellow block from on top of the orange block\\nstack the yellow block on top of the red block', answer={'init': 'the red block is clear, the yellow block is clear, the hand is empty, the orange block is on top of the blue block, the yellow block is on top of the orange block, the red block is on the table and the blue block is on the table', 'goal': 'the orange block is on top of the blue block and the yellow block is on top of the red block', 'plan': '\\nunstack the yellow block from on top of the orange block\\nstack the yellow block on top of the red block\\n[PLAN END]\\n', 'question': '\\n[STATEMENT]\\nAs initial conditions I have that, the red block is clear, the yellow block is clear, the hand is empty, the orange block is on top of the blue block, the yellow block is on top of the orange block, the red block is on the table and the blue block is on the table.\\nMy goal is to have that the orange block is on top of the blue block and the yellow block is on top of the red block.\\n\\nMy plan is as follows:\\n\\n[PLAN]\\n', 'instance_file': 'LLMs-Planning/llm_planning_analysis/instances/blocksworld/generated_basic/instance-192.pddl'};accuracy=1.000 (1/1)\n",
            "blocksworld:   2% 1/47 [03:38<2:47:30, 218.48s/it]\n",
            "MCTS iteration:   0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "MCTS iteration:  10% 1/10 [01:27<13:10, 87.81s/it]\u001b[A\n",
            "                                                  \u001b[A[+]: Saving plan in tmp_plan.txt\n",
            "RESPONSE::: Checking plan: tmp_plan.txt\n",
            "Plan executed successfully - checking goal\n",
            "Plan valid\n",
            "Final value: 2 \n",
            "\n",
            "Successful plans:\n",
            "Value: 2\n",
            " tmp_plan.txt 2 \n",
            "\n",
            "\n",
            "Case #2: correct=True, output='pick up the blue block\\nstack the blue block on top of the orange block', answer={'init': 'the blue block is clear, the orange block is clear, the yellow block is clear, the hand is empty, the yellow block is on top of the red block, the red block is on the table, the blue block is on the table and the orange block is on the table', 'goal': 'the blue block is on top of the orange block and the yellow block is on top of the red block', 'plan': '\\npick up the blue block\\nstack the blue block on top of the orange block\\n[PLAN END]\\n', 'question': '\\n[STATEMENT]\\nAs initial conditions I have that, the blue block is clear, the orange block is clear, the yellow block is clear, the hand is empty, the yellow block is on top of the red block, the red block is on the table, the blue block is on the table and the orange block is on the table.\\nMy goal is to have that the blue block is on top of the orange block and the yellow block is on top of the red block.\\n\\nMy plan is as follows:\\n\\n[PLAN]\\n', 'instance_file': 'LLMs-Planning/llm_planning_analysis/instances/blocksworld/generated_basic/instance-149.pddl'};accuracy=1.000 (2/2)\n",
            "blocksworld:   4% 2/47 [05:06<1:46:13, 141.64s/it]\n",
            "MCTS iteration:   0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "MCTS iteration:  10% 1/10 [03:22<30:22, 202.47s/it]\u001b[A"
          ]
        }
      ],
      "source": [
        "# /content/llm-reasoners_tutorial/reasoners/lm/hf_model.py の98行目をコメントアウトする\n",
        "# assets/llm-reasoners_tutorial/examples/rap_blocksworld/inference.pyの235行目をfire.Fire(llama_hf_main)に変更\n",
        "# prompt_pathをpool_prompt_v2_step_*.jsonにする\n",
        "# rap_inference.pyのrap_を消す\n",
        "\n",
        "!python examples/blocksworld/rap_inference.py --data_path 'examples/blocksworld/data/full_data/step_2.json' --prompt_path 'examples/blocksworld/prompts/pool_prompt_v2_step_2.json' --depth_limit 6 --llama_path \"meta-llama/Llama-2-7b-hf\" --peft_path None --batch_size 2 --quantized 'nf4' --output_trace_in_each_iter\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}